apiVersion: projectriff.io/v1alpha1
kind: Manifest
metadata:
  creationTimestamp: null
  name: riff-install
  namespace: kube-system
spec:
  resources:
  - content: |+
      ---
      # PATCH #1: Creating the istio-system namespace.
      apiVersion: v1
      kind: Namespace
      metadata:
        name: istio-system
        labels:
          istio-injection: disabled
      # PATCH #1 ends.
      ---
      # Source: istio/charts/gateways/templates/poddisruptionbudget.yaml

      apiVersion: policy/v1beta1
      kind: PodDisruptionBudget
      metadata:
        name: cluster-local-gateway
        namespace: istio-system
        labels:
          chart: gateways
          heritage: Tiller
          release: release-name
          app: cluster-local-gateway
          istio: cluster-local-gateway
      spec:

        minAvailable: 1
        selector:
          matchLabels:
            release: release-name
            app: cluster-local-gateway
            istio: cluster-local-gateway
      ---
      apiVersion: policy/v1beta1
      kind: PodDisruptionBudget
      metadata:
        name: istio-ingressgateway
        namespace: istio-system
        labels:
          chart: gateways
          heritage: Tiller
          release: release-name
          app: istio-ingressgateway
          istio: ingressgateway
      spec:

        minAvailable: 1
        selector:
          matchLabels:
            release: release-name
            app: istio-ingressgateway
            istio: ingressgateway
      ---

      ---
      # Source: istio/charts/pilot/templates/poddisruptionbudget.yaml

      apiVersion: policy/v1beta1
      kind: PodDisruptionBudget
      metadata:
        name: istio-pilot
        namespace: istio-system
        labels:
          app: pilot
          chart: pilot
          heritage: Tiller
          release: release-name
          istio: pilot
      spec:

        minAvailable: 1
        selector:
          matchLabels:
            app: pilot
            release: release-name
            istio: pilot

      ---
      # Source: istio/templates/configmap.yaml

      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: istio
        namespace: istio-system
        labels:
          app: istio
          chart: istio
          heritage: Tiller
          release: release-name
      data:
        mesh: |-
          # Set the following variable to true to disable policy checks by the Mixer.
          # Note that metrics will still be reported to the Mixer.
          disablePolicyChecks: true

          # Set enableTracing to false to disable request tracing.
          enableTracing: true

          # Set accessLogFile to empty string to disable access log.
          accessLogFile: "/dev/stdout"

          # If accessLogEncoding is TEXT, value will be used directly as the log format
          # example: "[%START_TIME%] %REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\n"
          # If AccessLogEncoding is JSON, value will be parsed as map[string]string
          # example: '{"start_time": "%START_TIME%", "req_method": "%REQ(:METHOD)%"}'
          # Leave empty to use default log format
          accessLogFormat: ""

          # Set accessLogEncoding to JSON or TEXT to configure sidecar access log
          accessLogEncoding: 'TEXT'
          # Let Pilot give ingresses the public IP of the Istio ingressgateway
          ingressService: istio-ingressgateway

          # Default connect timeout for dynamic clusters generated by Pilot and returned via XDS
          connectTimeout: 10s

          # DNS refresh rate for Envoy clusters of type STRICT_DNS
          dnsRefreshRate: 5s

          # Unix Domain Socket through which envoy communicates with NodeAgent SDS to get
          # key/cert for mTLS. Use secret-mount files instead of SDS if set to empty.
          sdsUdsPath:

          # This flag is used by secret discovery service(SDS).
          # If set to true(prerequisite: https://kubernetes.io/docs/concepts/storage/volumes/#projected), Istio will inject volumes mount
          # for k8s service account JWT, so that K8s API server mounts k8s service account JWT to envoy container, which
          # will be used to generate key/cert eventually. This isn't supported for non-k8s case.
          enableSdsTokenMount: false

          # This flag is used by secret discovery service(SDS).
          # If set to true, envoy will fetch normal k8s service account JWT from '/var/run/secrets/kubernetes.io/serviceaccount/token'
          # (https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#accessing-the-api-from-a-pod)
          # and pass to sds server, which will be used to request key/cert eventually.
          # this flag is ignored if enableSdsTokenMount is set.
          # This isn't supported for non-k8s case.
          sdsUseK8sSaJwt: false

          # The trust domain corresponds to the trust root of a system.
          # Refer to https://github.com/spiffe/spiffe/blob/master/standards/SPIFFE-ID.md#21-trust-domain
          trustDomain:

          # Set the default behavior of the sidecar for handling outbound traffic from the application:
          # ALLOW_ANY - outbound traffic to unknown destinations will be allowed, in case there are no
          #   services or ServiceEntries for the destination port
          # REGISTRY_ONLY - restrict outbound traffic to services defined in the service registry as well
          #   as those defined through ServiceEntries
          outboundTrafficPolicy:
            mode: ALLOW_ANY

          localityLbSetting:
            {}


          # The namespace to treat as the administrative root namespace for istio
          # configuration.
          rootNamespace: istio-system

          defaultConfig:
            #
            # TCP connection timeout between Envoy & the application, and between Envoys.  Used for static clusters
            # defined in Envoy's configuration file
            connectTimeout: 10s
            #
            ### ADVANCED SETTINGS #############
            # Where should envoy's configuration be stored in the istio-proxy container
            configPath: "/etc/istio/proxy"
            binaryPath: "/usr/local/bin/envoy"
            # The pseudo service name used for Envoy.
            serviceCluster: istio-proxy
            # These settings that determine how long an old Envoy
            # process should be kept alive after an occasional reload.
            drainDuration: 45s
            parentShutdownDuration: 1m0s
            #
            # The mode used to redirect inbound connections to Envoy. This setting
            # has no effect on outbound traffic: iptables REDIRECT is always used for
            # outbound connections.
            # If "REDIRECT", use iptables REDIRECT to NAT and redirect to Envoy.
            # The "REDIRECT" mode loses source addresses during redirection.
            # If "TPROXY", use iptables TPROXY to redirect to Envoy.
            # The "TPROXY" mode preserves both the source and destination IP
            # addresses and ports, so that they can be used for advanced filtering
            # and manipulation.
            # The "TPROXY" mode also configures the sidecar to run with the
            # CAP_NET_ADMIN capability, which is required to use TPROXY.
            #interceptionMode: REDIRECT
            #
            # Port where Envoy listens (on local host) for admin commands
            # You can exec into the istio-proxy container in a pod and
            # curl the admin port (curl http://localhost:15000/) to obtain
            # diagnostic information from Envoy. See
            # https://lyft.github.io/envoy/docs/operations/admin.html
            # for more details
            proxyAdminPort: 15000
            #
            # Set concurrency to a specific number to control the number of Proxy worker threads.
            # If set to 0 (default), then start worker thread for each CPU thread/core.
            concurrency: 2
            #
            tracing:
              zipkin:
                # Address of the Zipkin collector
                address: zipkin.istio-system:9411
            #
            # Mutual TLS authentication between sidecars and istio control plane.
            controlPlaneAuthPolicy: NONE
            #
            # Address where istio Pilot service is running
            discoveryAddress: istio-pilot.istio-system:15010

        # Configuration file for the mesh networks to be used by the Split Horizon EDS.
        meshNetworks: |-
          networks: {}

      ---
      # Source: istio/charts/gateways/templates/serviceaccount.yaml

      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: cluster-local-gateway-service-account
        namespace: istio-system
        labels:
          app: cluster-local-gateway
          chart: gateways
          heritage: Tiller
          release: release-name
      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: istio-ingressgateway-service-account
        namespace: istio-system
        labels:
          app: istio-ingressgateway
          chart: gateways
          heritage: Tiller
          release: release-name
      ---


      ---
      # Source: istio/charts/pilot/templates/serviceaccount.yaml
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: istio-pilot-service-account
        namespace: istio-system
        labels:
          app: pilot
          chart: pilot
          heritage: Tiller
          release: release-name

      ---
      # Source: istio/templates/serviceaccount.yaml
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: istio-multi
        namespace: istio-system

      ---
      # Source: istio/charts/gateways/templates/clusterrole.yaml

      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: cluster-local-gateway-istio-system
        labels:
          app: cluster-local-gateway
          chart: gateways
          heritage: Tiller
          release: release-name
      rules:
      - apiGroups: ["networking.istio.io"]
        resources: ["virtualservices", "destinationrules", "gateways"]
        verbs: ["get", "watch", "list", "update"]
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: istio-ingressgateway-istio-system
        labels:
          app: ingressgateway
          chart: gateways
          heritage: Tiller
          release: release-name
      rules:
      - apiGroups: ["networking.istio.io"]
        resources: ["virtualservices", "destinationrules", "gateways"]
        verbs: ["get", "watch", "list", "update"]
      ---

      ---
      # Source: istio/charts/pilot/templates/clusterrole.yaml
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: istio-pilot-istio-system
        labels:
          app: pilot
          chart: pilot
          heritage: Tiller
          release: release-name
      rules:
      - apiGroups: ["config.istio.io"]
        resources: ["*"]
        verbs: ["*"]
      - apiGroups: ["rbac.istio.io"]
        resources: ["*"]
        verbs: ["get", "watch", "list"]
      - apiGroups: ["networking.istio.io"]
        resources: ["*"]
        verbs: ["*"]
      - apiGroups: ["authentication.istio.io"]
        resources: ["*"]
        verbs: ["*"]
      - apiGroups: ["apiextensions.k8s.io"]
        resources: ["customresourcedefinitions"]
        verbs: ["*"]
      - apiGroups: ["extensions"]
        resources: ["ingresses", "ingresses/status"]
        verbs: ["*"]
      - apiGroups: [""]
        resources: ["configmaps"]
        verbs: ["create", "get", "list", "watch", "update"]
      - apiGroups: [""]
        resources: ["endpoints", "pods", "services", "namespaces", "nodes", "secrets"]
        verbs: ["get", "list", "watch"]

      ---
      # Source: istio/templates/clusterrole.yaml
      kind: ClusterRole
      apiVersion: rbac.authorization.k8s.io/v1
      metadata:
        name: istio-reader
      rules:
        - apiGroups: ['']
          resources: ['nodes', 'pods', 'services', 'endpoints', "replicationcontrollers"]
          verbs: ['get', 'watch', 'list']
        - apiGroups: ["extensions", "apps"]
          resources: ["replicasets"]
          verbs: ["get", "list", "watch"]

      ---
      # Source: istio/charts/gateways/templates/clusterrolebindings.yaml

      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: cluster-local-gateway-istio-system
        labels:
          app: cluster-local-gateway
          chart: gateways
          heritage: Tiller
          release: release-name
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: cluster-local-gateway-istio-system
      subjects:
      - kind: ServiceAccount
        name: cluster-local-gateway-service-account
        namespace: istio-system
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: istio-ingressgateway-istio-system
        labels:
          app: ingressgateway
          chart: gateways
          heritage: Tiller
          release: release-name
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: istio-ingressgateway-istio-system
      subjects:
      - kind: ServiceAccount
        name: istio-ingressgateway-service-account
        namespace: istio-system
      ---

      ---
      # Source: istio/charts/pilot/templates/clusterrolebinding.yaml
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: istio-pilot-istio-system
        labels:
          app: pilot
          chart: pilot
          heritage: Tiller
          release: release-name
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: istio-pilot-istio-system
      subjects:
        - kind: ServiceAccount
          name: istio-pilot-service-account
          namespace: istio-system

      ---
      # Source: istio/templates/clusterrolebinding.yaml
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: istio-multi
        labels:
          chart: istio-1.1.0
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: istio-reader
      subjects:
      - kind: ServiceAccount
        name: istio-multi
        namespace: istio-system

      ---
      # Source: istio/charts/gateways/templates/role.yaml

      apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      metadata:
        name: istio-ingressgateway-sds
        namespace: istio-system
      rules:
      - apiGroups: [""]
        resources: ["secrets"]
        verbs: ["get", "watch", "list"]
      ---

      ---
      # Source: istio/charts/gateways/templates/rolebindings.yaml

      apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      metadata:
        name: istio-ingressgateway-sds
        namespace: istio-system
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: Role
        name: istio-ingressgateway-sds
      subjects:
      - kind: ServiceAccount
        name: istio-ingressgateway-service-account
      ---

      ---
      # Source: istio/charts/gateways/templates/service.yaml

      apiVersion: v1
      kind: Service
      metadata:
        name: cluster-local-gateway
        namespace: istio-system
        annotations:
        labels:
          chart: gateways
          heritage: Tiller
          release: release-name
          app: cluster-local-gateway
          istio: cluster-local-gateway
      spec:
        type: ClusterIP
        selector:
          release: release-name
          app: cluster-local-gateway
          istio: cluster-local-gateway
        ports:
          -
            name: status-port
            port: 15020
          -
            name: http2
            port: 80
          -
            name: https
            port: 443
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: istio-ingressgateway
        namespace: istio-system
        annotations:
        labels:
          chart: gateways
          heritage: Tiller
          release: release-name
          app: istio-ingressgateway
          istio: ingressgateway
      spec:
        type: LoadBalancer
        selector:
          release: release-name
          app: istio-ingressgateway
          istio: ingressgateway
        ports:
          -
            name: status-port
            port: 15020
          -
            name: http2
            port: 80
          -
            name: https
            port: 443
      ---

      ---
      # Source: istio/charts/pilot/templates/service.yaml
      apiVersion: v1
      kind: Service
      metadata:
        name: istio-pilot
        namespace: istio-system
        labels:
          app: pilot
          chart: pilot
          heritage: Tiller
          release: release-name
          istio: pilot
      spec:
        ports:
        - port: 15010
          name: grpc-xds # direct
        - port: 15011
          name: https-xds # mTLS
        - port: 8080
          name: http-legacy-discovery # direct
        - port: 15014
          name: http-monitoring
        selector:
          istio: pilot

      ---
      # Source: istio/charts/gateways/templates/deployment.yaml

      apiVersion: extensions/v1beta1
      kind: Deployment
      metadata:
        name: cluster-local-gateway
        namespace: istio-system
        labels:
          chart: gateways
          heritage: Tiller
          release: release-name
          app: cluster-local-gateway
          istio: cluster-local-gateway
      spec:
        replicas: 1
        template:
          metadata:
            labels:
              chart: gateways
              heritage: Tiller
              release: release-name
              app: cluster-local-gateway
              istio: cluster-local-gateway
            annotations:
              sidecar.istio.io/inject: "false"
          spec:
            serviceAccountName: cluster-local-gateway-service-account
            containers:
              - name: istio-proxy
                image: "docker.io/istio/proxyv2:1.1.7@sha256:e6f039115c7d5ef9c8f6b049866fbf9b6f5e2255d3a733bb8756b36927749822"
                imagePullPolicy: IfNotPresent
                ports:
                  - containerPort: 15020
                  - containerPort: 80
                  - containerPort: 443
                  - containerPort: 15090
                    protocol: TCP
                    name: http-envoy-prom
                args:
                - proxy
                - router
                - --domain
                - $(POD_NAMESPACE).svc.cluster.local
                - --log_output_level=default:info
                - --drainDuration
                - '45s' #drainDuration
                - --parentShutdownDuration
                - '1m0s' #parentShutdownDuration
                - --connectTimeout
                - '10s' #connectTimeout
                - --serviceCluster
                - cluster-local-gateway
                - --zipkinAddress
                - zipkin:9411
                - --proxyAdminPort
                - "15000"
                - --statusPort
                - "15020"
                - --controlPlaneAuthPolicy
                - NONE
                - --discoveryAddress
                - istio-pilot:15010
                readinessProbe:
                  failureThreshold: 30
                  httpGet:
                    path: /healthz/ready
                    port: 15020
                    scheme: HTTP
                  initialDelaySeconds: 1
                  periodSeconds: 2
                  successThreshold: 1
                  timeoutSeconds: 1
                resources:
                  requests:
                    cpu: 10m

                env:
                - name: POD_NAME
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: metadata.name
                - name: POD_NAMESPACE
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: metadata.namespace
                - name: INSTANCE_IP
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: status.podIP
                - name: HOST_IP
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: status.hostIP
                - name: ISTIO_META_POD_NAME
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: metadata.name
                - name: ISTIO_META_CONFIG_NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
                volumeMounts:
                - name: istio-certs
                  mountPath: /etc/certs
                  readOnly: true
                - name: cluster-local-gateway-certs
                  mountPath: "/etc/istio/cluster-local-gateway-certs"
                  readOnly: true
                - name: cluster-local-gateway-ca-certs
                  mountPath: "/etc/istio/cluster-local-gateway-ca-certs"
                  readOnly: true
            volumes:
            - name: istio-certs
              secret:
                secretName: istio.cluster-local-gateway-service-account
                optional: true
            - name: cluster-local-gateway-certs
              secret:
                secretName: "istio-cluster-local-gateway-certs"
                optional: true
            - name: cluster-local-gateway-ca-certs
              secret:
                secretName: "istio-cluster-local-gateway-ca-certs"
                optional: true
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: beta.kubernetes.io/arch
                      operator: In
                      values:
                      - amd64
                      - ppc64le
                      - s390x
                preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 2
                  preference:
                    matchExpressions:
                    - key: beta.kubernetes.io/arch
                      operator: In
                      values:
                      - amd64
                - weight: 2
                  preference:
                    matchExpressions:
                    - key: beta.kubernetes.io/arch
                      operator: In
                      values:
                      - ppc64le
                - weight: 2
                  preference:
                    matchExpressions:
                    - key: beta.kubernetes.io/arch
                      operator: In
                      values:
                      - s390x
      ---
      apiVersion: extensions/v1beta1
      kind: Deployment
      metadata:
        name: istio-ingressgateway
        namespace: istio-system
        labels:
          chart: gateways
          heritage: Tiller
          release: release-name
          app: istio-ingressgateway
          istio: ingressgateway
      spec:
        template:
          metadata:
            labels:
              chart: gateways
              heritage: Tiller
              release: release-name
              app: istio-ingressgateway
              istio: ingressgateway
            annotations:
              sidecar.istio.io/inject: "false"
          spec:
            serviceAccountName: istio-ingressgateway-service-account
            containers:
              - name: ingress-sds
                image: "docker.io/istio/node-agent-k8s:1.1.7@sha256:5de4f4b29193003f423884f140b7df0c504ae184705b1a75041877072f5ab518"
                imagePullPolicy: IfNotPresent
                resources:
                  limits:
                    cpu: 2000m
                    memory: 1024Mi
                  requests:
                    cpu: 100m
                    memory: 128Mi

                env:
                - name: "ENABLE_WORKLOAD_SDS"
                  value: "false"
                - name: "ENABLE_INGRESS_GATEWAY_SDS"
                  value: "true"
                - name: "INGRESS_GATEWAY_NAMESPACE"
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: metadata.namespace
                volumeMounts:
                - name: ingressgatewaysdsudspath
                  mountPath: /var/run/ingress_gateway
              - name: istio-proxy
                image: "docker.io/istio/proxyv2:1.1.7@sha256:e6f039115c7d5ef9c8f6b049866fbf9b6f5e2255d3a733bb8756b36927749822"
                imagePullPolicy: IfNotPresent
                ports:
                  - containerPort: 15020
                  - containerPort: 80
                  - containerPort: 443
                  - containerPort: 15090
                    protocol: TCP
                    name: http-envoy-prom
                args:
                - proxy
                - router
                - --domain
                - $(POD_NAMESPACE).svc.cluster.local
                - --log_output_level=default:info
                - --drainDuration
                - '45s' #drainDuration
                - --parentShutdownDuration
                - '1m0s' #parentShutdownDuration
                - --connectTimeout
                - '10s' #connectTimeout
                - --serviceCluster
                - istio-ingressgateway
                - --zipkinAddress
                - zipkin:9411
                - --proxyAdminPort
                - "15000"
                - --statusPort
                - "15020"
                - --controlPlaneAuthPolicy
                - NONE
                - --discoveryAddress
                - istio-pilot:15010
                readinessProbe:
                  failureThreshold: 30
                  httpGet:
                    path: /healthz/ready
                    port: 15020
                    scheme: HTTP
                  initialDelaySeconds: 1
                  periodSeconds: 2
                  successThreshold: 1
                  timeoutSeconds: 1
                resources:
                  limits:
                    cpu: 2000m
                    memory: 1024Mi
                  requests:
                    cpu: 100m
                    memory: 128Mi

                env:
                - name: POD_NAME
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: metadata.name
                - name: POD_NAMESPACE
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: metadata.namespace
                - name: INSTANCE_IP
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: status.podIP
                - name: HOST_IP
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: status.hostIP
                - name: ISTIO_META_POD_NAME
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: metadata.name
                - name: ISTIO_META_CONFIG_NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
                - name: ISTIO_META_USER_SDS
                  value: "true"
                - name: ISTIO_META_ROUTER_MODE
                  value: sni-dnat
                volumeMounts:
                - name: ingressgatewaysdsudspath
                  mountPath: /var/run/ingress_gateway
                - name: istio-certs
                  mountPath: /etc/certs
                  readOnly: true
                - name: ingressgateway-certs
                  mountPath: "/etc/istio/ingressgateway-certs"
                  readOnly: true
                - name: ingressgateway-ca-certs
                  mountPath: "/etc/istio/ingressgateway-ca-certs"
                  readOnly: true
            volumes:
            - name: ingressgatewaysdsudspath
              emptyDir: {}
            - name: istio-certs
              secret:
                secretName: istio.istio-ingressgateway-service-account
                optional: true
            - name: ingressgateway-certs
              secret:
                secretName: "istio-ingressgateway-certs"
                optional: true
            - name: ingressgateway-ca-certs
              secret:
                secretName: "istio-ingressgateway-ca-certs"
                optional: true
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: beta.kubernetes.io/arch
                      operator: In
                      values:
                      - amd64
                      - ppc64le
                      - s390x
                preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 2
                  preference:
                    matchExpressions:
                    - key: beta.kubernetes.io/arch
                      operator: In
                      values:
                      - amd64
                - weight: 2
                  preference:
                    matchExpressions:
                    - key: beta.kubernetes.io/arch
                      operator: In
                      values:
                      - ppc64le
                - weight: 2
                  preference:
                    matchExpressions:
                    - key: beta.kubernetes.io/arch
                      operator: In
                      values:
                      - s390x
      ---

      ---
      # Source: istio/charts/pilot/templates/deployment.yaml
      apiVersion: extensions/v1beta1
      kind: Deployment
      metadata:
        name: istio-pilot
        namespace: istio-system
        # TODO: default template doesn't have this, which one is right ?
        labels:
          app: pilot
          chart: pilot
          heritage: Tiller
          release: release-name
          istio: pilot
        annotations:
          checksum/config-volume: f8da08b6b8c170dde721efd680270b2901e750d4aa186ebb6c22bef5b78a43f9
      spec:
        strategy:
          rollingUpdate:
            maxSurge: 1
            maxUnavailable: 0
        selector:
          matchLabels:
            istio: pilot
        template:
          metadata:
            labels:
              app: pilot
              chart: pilot
              heritage: Tiller
              release: release-name
              istio: pilot
            annotations:
              sidecar.istio.io/inject: "false"
          spec:
            serviceAccountName: istio-pilot-service-account
            containers:
              - name: discovery
                image: "docker.io/istio/pilot:1.1.7@sha256:428959abad1e61f4adbfb3b35ec61d85c952c7c46696e15adc7b3863eb9719f5"
                imagePullPolicy: IfNotPresent
                args:
                - "discovery"
                - --monitoringAddr=:15014
                - --log_output_level=default:info
                - --domain
                - cluster.local
                - --secureGrpcAddr
                - ""
                - --keepaliveMaxServerConnectionAge
                - "30m"
                ports:
                - containerPort: 8080
                - containerPort: 15010
                - containerPort: 15011
                readinessProbe:
                  httpGet:
                    path: /ready
                    port: 8080
                  initialDelaySeconds: 5
                  periodSeconds: 30
                  timeoutSeconds: 5
                env:
                - name: POD_NAME
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: metadata.name
                - name: POD_NAMESPACE
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: metadata.namespace
                - name: GODEBUG
                  value: "gctrace=1"
                - name: PILOT_PUSH_THROTTLE
                  value: "100"
                - name: PILOT_TRACE_SAMPLING
                  value: "100"
                - name: PILOT_DISABLE_XDS_MARSHALING_TO_ANY
                  value: "1"
                resources:
                  requests:
                    cpu: 100m
                    memory: 128Mi

                volumeMounts:
                - name: config-volume
                  mountPath: /etc/istio/config
                - name: istio-certs
                  mountPath: /etc/certs
                  readOnly: true
            volumes:
            - name: config-volume
              configMap:
                name: istio
            - name: istio-certs
              secret:
                secretName: istio.istio-pilot-service-account
                optional: true
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: beta.kubernetes.io/arch
                      operator: In
                      values:
                      - amd64
                      - ppc64le
                      - s390x
                preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 2
                  preference:
                    matchExpressions:
                    - key: beta.kubernetes.io/arch
                      operator: In
                      values:
                      - amd64
                - weight: 2
                  preference:
                    matchExpressions:
                    - key: beta.kubernetes.io/arch
                      operator: In
                      values:
                      - ppc64le
                - weight: 2
                  preference:
                    matchExpressions:
                    - key: beta.kubernetes.io/arch
                      operator: In
                      values:
                      - s390x

      ---
      # Source: istio/charts/gateways/templates/autoscale.yaml

      apiVersion: autoscaling/v2beta1
      kind: HorizontalPodAutoscaler
      metadata:
        name: istio-ingressgateway
        namespace: istio-system
        labels:
          app: ingressgateway
          chart: gateways
          heritage: Tiller
          release: release-name
      spec:
        maxReplicas: 1
        minReplicas: 1
        scaleTargetRef:
          apiVersion: apps/v1beta1
          kind: Deployment
          name: istio-ingressgateway
        metrics:
          - type: Resource
            resource:
              name: cpu
              targetAverageUtilization: 80
      ---

      ---
      # Source: istio/charts/pilot/templates/autoscale.yaml

      apiVersion: autoscaling/v2beta1
      kind: HorizontalPodAutoscaler
      metadata:
        name: istio-pilot
        namespace: istio-system
        labels:
          app: pilot
          chart: pilot
          heritage: Tiller
          release: release-name
      spec:
        maxReplicas: 5
        minReplicas: 1
        scaleTargetRef:
          apiVersion: apps/v1beta1
          kind: Deployment
          name: istio-pilot
        metrics:
        - type: Resource
          resource:
            name: cpu
            targetAverageUtilization: 80
      ---

      ---
      # Source: istio/charts/gateways/templates/preconfigured.yaml


      ---
      # Source: istio/charts/mixer/templates/autoscale.yaml


      ---
      # Source: istio/charts/mixer/templates/clusterrole.yaml


      ---
      # Source: istio/charts/mixer/templates/clusterrolebinding.yaml


      ---
      # Source: istio/charts/mixer/templates/config.yaml


      ---
      # Source: istio/charts/mixer/templates/deployment.yaml


      ---
      # Source: istio/charts/mixer/templates/poddisruptionbudget.yaml


      ---
      # Source: istio/charts/mixer/templates/service.yaml



      ---
      # Source: istio/charts/mixer/templates/serviceaccount.yaml


      ---
      # Source: istio/charts/pilot/templates/meshexpansion.yaml



      ---
      # Source: istio/templates/endpoints.yaml


      ---
      # Source: istio/templates/install-custom-resources.sh.tpl


      ---
      # Source: istio/templates/service.yaml


      ---
      # Source: istio/templates/sidecar-injector-configmap.yaml


    name: istio
    path: https://storage.googleapis.com/projectriff/istio/istio-v1.1.7-lean-riff.yaml
  - content: |
      apiVersion: v1
      kind: Namespace
      metadata:
        name: knative-build

      ---
      apiVersion: policy/v1beta1
      kind: PodSecurityPolicy
      metadata:
        name: knative-build
      spec:
        allowPrivilegeEscalation: false
        fsGroup:
          ranges:
          - max: 65535
            min: 1
          rule: MustRunAs
        hostIPC: false
        hostNetwork: false
        hostPID: false
        privileged: false
        runAsUser:
          rule: RunAsAny
        seLinux:
          rule: RunAsAny
        supplementalGroups:
          ranges:
          - max: 65535
            min: 1
          rule: MustRunAs
        volumes:
        - configMap
        - secret

      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: knative-build-admin
      rules:
      - apiGroups:
        - ""
        resources:
        - pods
        - namespaces
        - secrets
        - events
        - serviceaccounts
        - configmaps
        verbs:
        - get
        - list
        - create
        - update
        - delete
        - patch
        - watch
      - apiGroups:
        - apps
        resources:
        - deployments
        - deployments/finalizers
        verbs:
        - get
        - list
        - create
        - update
        - delete
        - patch
        - watch
      - apiGroups:
        - admissionregistration.k8s.io
        resources:
        - mutatingwebhookconfigurations
        verbs:
        - get
        - list
        - create
        - update
        - delete
        - patch
        - watch
      - apiGroups:
        - apiextensions.k8s.io
        resources:
        - customresourcedefinitions
        verbs:
        - get
        - list
        - create
        - update
        - delete
        - patch
        - watch
      - apiGroups:
        - build.knative.dev
        resources:
        - builds
        - buildtemplates
        - clusterbuildtemplates
        verbs:
        - get
        - list
        - create
        - update
        - delete
        - patch
        - watch
      - apiGroups:
        - build.knative.dev
        resources:
        - builds/status
        - buildtemplates/status
        - clusterbuildtemplates/status
        verbs:
        - get
        - list
        - create
        - update
        - delete
        - patch
        - watch
      - apiGroups:
        - caching.internal.knative.dev
        resources:
        - images
        verbs:
        - get
        - list
        - create
        - update
        - delete
        - deletecollection
        - patch
        - watch
      - apiGroups:
        - policy
        resourceNames:
        - knative-build
        resources:
        - podsecuritypolicies
        verbs:
        - use

      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: build-controller
        namespace: knative-build

      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: build-controller-admin
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: knative-build-admin
      subjects:
      - kind: ServiceAccount
        name: build-controller
        namespace: knative-build

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          knative.dev/crd-install: "true"
        name: builds.build.knative.dev
      spec:
        additionalPrinterColumns:
        - JSONPath: .status.conditions[?(@.type=="Succeeded")].status
          name: Succeeded
          type: string
        - JSONPath: .status.conditions[?(@.type=="Succeeded")].reason
          name: Reason
          type: string
        - JSONPath: .status.startTime
          name: StartTime
          type: date
        - JSONPath: .status.completionTime
          name: CompletionTime
          type: date
        group: build.knative.dev
        names:
          categories:
          - all
          - knative
          kind: Build
          plural: builds
        scope: Namespaced
        subresources:
          status: {}
        version: v1alpha1

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          knative.dev/crd-install: "true"
        name: buildtemplates.build.knative.dev
      spec:
        additionalPrinterColumns:
        - JSONPath: .metadata.creationTimestamp
          name: Age
          type: date
        group: build.knative.dev
        names:
          categories:
          - all
          - knative
          kind: BuildTemplate
          plural: buildtemplates
        scope: Namespaced
        subresources:
          status: {}
        version: v1alpha1

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          knative.dev/crd-install: "true"
        name: clusterbuildtemplates.build.knative.dev
      spec:
        additionalPrinterColumns:
        - JSONPath: .metadata.creationTimestamp
          name: Age
          type: date
        group: build.knative.dev
        names:
          categories:
          - all
          - knative
          kind: ClusterBuildTemplate
          plural: clusterbuildtemplates
        scope: Cluster
        subresources:
          status: {}
        version: v1alpha1

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          knative.dev/crd-install: "true"
        name: images.caching.internal.knative.dev
      spec:
        group: caching.internal.knative.dev
        names:
          categories:
          - all
          - knative-internal
          - caching
          kind: Image
          plural: images
          shortNames:
          - img
          singular: image
        scope: Namespaced
        subresources:
          status: {}
        version: v1alpha1

      ---
      apiVersion: v1
      kind: Service
      metadata:
        labels:
          app: build-controller
        name: build-controller
        namespace: knative-build
      spec:
        ports:
        - name: metrics
          port: 9090
          protocol: TCP
          targetPort: 9090
        selector:
          app: build-controller

      ---
      apiVersion: v1
      kind: Service
      metadata:
        labels:
          role: build-webhook
        name: build-webhook
        namespace: knative-build
      spec:
        ports:
        - port: 443
          targetPort: 8443
        selector:
          role: build-webhook

      ---
      apiVersion: caching.internal.knative.dev/v1alpha1
      kind: Image
      metadata:
        name: creds-init
        namespace: knative-build
      spec:
        image: gcr.io/knative-releases/github.com/knative/build/cmd/creds-init@sha256:1a984c032a2606f8491f4a19a85209dcc1ae2cfd494c3dafe8a74269379ad2c8
      ---
      apiVersion: caching.internal.knative.dev/v1alpha1
      kind: Image
      metadata:
        name: git-init
        namespace: knative-build
      spec:
        image: gcr.io/knative-releases/github.com/knative/build/cmd/git-init@sha256:06505d8c621e9337d0dd1bc13ed4545a33e857fbb6374740cc6337d2ba55796d
      ---
      apiVersion: caching.internal.knative.dev/v1alpha1
      kind: Image
      metadata:
        name: gcs-fetcher
        namespace: knative-build
      spec:
        image: gcr.io/cloud-builders/gcs-fetcher@sha256:31dfcc0555bec9744a207ffe8baf2131b2bbb82539fe8d31af3dde42c959f6c9
      ---
      apiVersion: caching.internal.knative.dev/v1alpha1
      kind: Image
      metadata:
        name: nop
        namespace: knative-build
      spec:
        image: gcr.io/knative-releases/github.com/knative/build/cmd/nop@sha256:8aca9c97ede9a550ac3536d00c5d7acaae5e3a4fe514f4329ec261d935eddabb

      ---
      apiVersion: v1
      data:
        loglevel.controller: info
        loglevel.creds-init: info
        loglevel.git-init: info
        loglevel.webhook: info
        zap-logger-config: |
          {
            "level": "info",
            "development": false,
            "sampling": {
              "initial": 100,
              "thereafter": 100
            },
            "outputPaths": ["stdout"],
            "errorOutputPaths": ["stderr"],
            "encoding": "json",
            "encoderConfig": {
              "timeKey": "",
              "levelKey": "level",
              "nameKey": "logger",
              "callerKey": "caller",
              "messageKey": "msg",
              "stacktraceKey": "stacktrace",
              "lineEnding": "",
              "levelEncoder": "",
              "timeEncoder": "",
              "durationEncoder": "",
              "callerEncoder": ""
            }
          }
      kind: ConfigMap
      metadata:
        name: config-logging
        namespace: knative-build

      ---
      apiVersion: v1
      data:
        _example: |
          ################################
          #                              #
          #    EXAMPLE CONFIGURATION     #
          #                              #
          ################################

          # This block is not actually functional configuration,
          # but serves to illustrate the available configuration
          # options and document them in a way that is accessible
          # to users that `kubectl edit` this config map.
          #
          # These sample configuration options may be copied out of
          # this example block and unindented to be in the data block
          # to actually change the configuration.

          # metrics.backend-destination field specifies the system metrics destination.
          # It supports either prometheus (the default) or stackdriver.
          # Note: Using stackdriver will incur additional charges
          metrics.backend-destination: prometheus

          # metrics.stackdriver-project-id field specifies the stackdriver project ID. This
          # field is optional. When running on GCE, application default credentials will be
          # used if this field is not provided.
          metrics.stackdriver-project-id: "<your stackdriver project id>"

          # metrics.allow-stackdriver-custom-metrics indicates whether it is allowed to send metrics to
          # Stackdriver using "global" resource type and custom metric type if the
          # metrics are not supported by "knative_revision" resource type. Setting this
          # flag to "true" could cause extra Stackdriver charge.
          # If metrics.backend-destination is not Stackdriver, this is ignored.
          metrics.allow-stackdriver-custom-metrics: "false"
      kind: ConfigMap
      metadata:
        name: config-observability
        namespace: knative-build

      ---
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: build-controller
        namespace: knative-build
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: build-controller
        template:
          metadata:
            labels:
              app: build-controller
          spec:
            containers:
            - args:
              - -logtostderr
              - -stderrthreshold
              - INFO
              - -creds-image
              - gcr.io/knative-releases/github.com/knative/build/cmd/creds-init@sha256:1a984c032a2606f8491f4a19a85209dcc1ae2cfd494c3dafe8a74269379ad2c8
              - -git-image
              - gcr.io/knative-releases/github.com/knative/build/cmd/git-init@sha256:06505d8c621e9337d0dd1bc13ed4545a33e857fbb6374740cc6337d2ba55796d
              - -nop-image
              - gcr.io/knative-releases/github.com/knative/build/cmd/nop@sha256:8aca9c97ede9a550ac3536d00c5d7acaae5e3a4fe514f4329ec261d935eddabb
              env:
              - name: SYSTEM_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
              - name: CONFIG_LOGGING_NAME
                value: config-logging
              - name: CONFIG_OBSERVABILITY_NAME
                value: config-observability
              - name: METRICS_DOMAIN
                value: knative.dev/build
              image: gcr.io/knative-releases/github.com/knative/build/cmd/controller@sha256:5adb5ba0647a7b1af1d90848bf72a75fa84efeb89e1d688465a2105c1cce1dc2
              name: build-controller
              ports:
              - containerPort: 9090
                name: metrics
              resources:
                limits:
                  cpu: 1000m
                  memory: 1000Mi
                requests:
                  cpu: 100m
                  memory: 100Mi
              volumeMounts:
              - mountPath: /etc/config-logging
                name: config-logging
            serviceAccountName: build-controller
            volumes:
            - configMap:
                name: config-logging
              name: config-logging

      ---
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: build-webhook
        namespace: knative-build
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: build-webhook
        template:
          metadata:
            labels:
              app: build-webhook
              role: build-webhook
          spec:
            containers:
            - args:
              - -logtostderr
              - -stderrthreshold
              - INFO
              env:
              - name: SYSTEM_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
              image: gcr.io/knative-releases/github.com/knative/build/cmd/webhook@sha256:35b1b5f72642e9c1ee71809fec309a019111beebf805f9ddddf154a97ad23975
              name: build-webhook
              resources:
                limits:
                  memory: 1000Mi
              volumeMounts:
              - mountPath: /etc/config-logging
                name: config-logging
            serviceAccountName: build-controller
            volumes:
            - configMap:
                name: config-logging
              name: config-logging

      ---
    name: knative-build
    path: https://storage.googleapis.com/knative-releases/build/previous/v0.7.0/build.yaml
  - content: |
      apiVersion: v1
      kind: Namespace
      metadata:
        labels:
          istio-injection: enabled
          serving.knative.dev/release: "v0.7.0"
        name: knative-serving

      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        labels:
          networking.knative.dev/certificate-provider: cert-manager
          serving.knative.dev/controller: "true"
          serving.knative.dev/release: "v0.7.0"
        name: knative-serving-certmanager
      rules:
      - apiGroups:
        - certmanager.k8s.io
        resources:
        - certificates
        verbs:
        - get
        - list
        - create
        - update
        - delete
        - patch
        - watch

      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        labels:
          networking.knative.dev/ingress-provider: istio
          serving.knative.dev/controller: "true"
          serving.knative.dev/release: "v0.7.0"
        name: knative-serving-istio
      rules:
      - apiGroups:
        - networking.istio.io
        resources:
        - virtualservices
        - gateways
        verbs:
        - get
        - list
        - create
        - update
        - delete
        - patch
        - watch

      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        labels:
          autoscaling.knative.dev/metric-provider: custom-metrics
          serving.knative.dev/release: "v0.7.0"
        name: custom-metrics-server-resources
      rules:
      - apiGroups:
        - custom.metrics.k8s.io
        resources:
        - '*'
        verbs:
        - '*'

      ---
      aggregationRule:
        clusterRoleSelectors:
        - matchLabels:
            serving.knative.dev/controller: "true"
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        labels:
          serving.knative.dev/release: "v0.7.0"
        name: knative-serving-admin
      rules: []
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        labels:
          serving.knative.dev/controller: "true"
          serving.knative.dev/release: "v0.7.0"
        name: knative-serving-core
      rules:
      - apiGroups:
        - ""
        resources:
        - pods
        - namespaces
        - secrets
        - configmaps
        - endpoints
        - services
        - events
        - serviceaccounts
        verbs:
        - get
        - list
        - create
        - update
        - delete
        - patch
        - watch
      - apiGroups:
        - ""
        resources:
        - endpoints/restricted
        verbs:
        - create
      - apiGroups:
        - apps
        resources:
        - deployments
        - deployments/finalizers
        verbs:
        - get
        - list
        - create
        - update
        - delete
        - patch
        - watch
      - apiGroups:
        - admissionregistration.k8s.io
        resources:
        - mutatingwebhookconfigurations
        verbs:
        - get
        - list
        - create
        - update
        - delete
        - patch
        - watch
      - apiGroups:
        - apiextensions.k8s.io
        resources:
        - customresourcedefinitions
        verbs:
        - get
        - list
        - create
        - update
        - delete
        - patch
        - watch
      - apiGroups:
        - autoscaling
        resources:
        - horizontalpodautoscalers
        verbs:
        - get
        - list
        - create
        - update
        - delete
        - patch
        - watch
      - apiGroups:
        - serving.knative.dev
        - autoscaling.internal.knative.dev
        - networking.internal.knative.dev
        resources:
        - '*'
        - '*/status'
        - '*/finalizers'
        verbs:
        - get
        - list
        - create
        - update
        - delete
        - deletecollection
        - patch
        - watch
      - apiGroups:
        - caching.internal.knative.dev
        resources:
        - images
        verbs:
        - get
        - list
        - create
        - update
        - delete
        - patch
        - watch

      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        labels:
          serving.knative.dev/release: "v0.7.0"
        name: controller
        namespace: knative-serving

      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        labels:
          autoscaling.knative.dev/metric-provider: custom-metrics
          serving.knative.dev/release: "v0.7.0"
        name: custom-metrics:system:auth-delegator
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: system:auth-delegator
      subjects:
      - kind: ServiceAccount
        name: controller
        namespace: knative-serving

      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        labels:
          autoscaling.knative.dev/metric-provider: custom-metrics
          serving.knative.dev/release: "v0.7.0"
        name: hpa-controller-custom-metrics
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: custom-metrics-server-resources
      subjects:
      - kind: ServiceAccount
        name: horizontal-pod-autoscaler
        namespace: kube-system

      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        labels:
          serving.knative.dev/release: "v0.7.0"
        name: knative-serving-controller-admin
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: knative-serving-admin
      subjects:
      - kind: ServiceAccount
        name: controller
        namespace: knative-serving

      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      metadata:
        labels:
          autoscaling.knative.dev/metric-provider: custom-metrics
          serving.knative.dev/release: "v0.7.0"
        name: custom-metrics-auth-reader
        namespace: kube-system
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: Role
        name: extension-apiserver-authentication-reader
      subjects:
      - kind: ServiceAccount
        name: controller
        namespace: knative-serving

      ---
      apiVersion: networking.istio.io/v1alpha3
      kind: Gateway
      metadata:
        labels:
          networking.knative.dev/ingress-provider: istio
          serving.knative.dev/release: "v0.7.0"
        name: knative-ingress-gateway
        namespace: knative-serving
      spec:
        selector:
          istio: ingressgateway
        servers:
        - hosts:
          - '*'
          port:
            name: http
            number: 80
            protocol: HTTP
        - hosts:
          - '*'
          port:
            name: https
            number: 443
            protocol: HTTPS
          tls:
            mode: PASSTHROUGH

      ---
      apiVersion: networking.istio.io/v1alpha3
      kind: Gateway
      metadata:
        labels:
          networking.knative.dev/ingress-provider: istio
          serving.knative.dev/release: "v0.7.0"
        name: cluster-local-gateway
        namespace: knative-serving
      spec:
        selector:
          istio: cluster-local-gateway
        servers:
        - hosts:
          - '*'
          port:
            name: http
            number: 80
            protocol: HTTP

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          knative.dev/crd-install: "true"
          serving.knative.dev/release: "v0.7.0"
        name: certificates.networking.internal.knative.dev
      spec:
        additionalPrinterColumns:
        - JSONPath: .status.conditions[?(@.type=="Ready")].status
          name: Ready
          type: string
        - JSONPath: .status.conditions[?(@.type=="Ready")].reason
          name: Reason
          type: string
        group: networking.internal.knative.dev
        names:
          categories:
          - all
          - knative-internal
          - networking
          kind: Certificate
          plural: certificates
          shortNames:
          - kcert
          singular: certificate
        scope: Namespaced
        subresources:
          status: {}
        version: v1alpha1

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          knative.dev/crd-install: "true"
          serving.knative.dev/release: "v0.7.0"
        name: clusteringresses.networking.internal.knative.dev
      spec:
        additionalPrinterColumns:
        - JSONPath: .status.conditions[?(@.type=='Ready')].status
          name: Ready
          type: string
        - JSONPath: .status.conditions[?(@.type=='Ready')].reason
          name: Reason
          type: string
        group: networking.internal.knative.dev
        names:
          categories:
          - all
          - knative-internal
          - networking
          kind: ClusterIngress
          plural: clusteringresses
          singular: clusteringress
        scope: Cluster
        subresources:
          status: {}
        version: v1alpha1

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          knative.dev/crd-install: "true"
          serving.knative.dev/release: "v0.7.0"
        name: configurations.serving.knative.dev
      spec:
        additionalPrinterColumns:
        - JSONPath: .status.latestCreatedRevisionName
          name: LatestCreated
          type: string
        - JSONPath: .status.latestReadyRevisionName
          name: LatestReady
          type: string
        - JSONPath: .status.conditions[?(@.type=='Ready')].status
          name: Ready
          type: string
        - JSONPath: .status.conditions[?(@.type=='Ready')].reason
          name: Reason
          type: string
        group: serving.knative.dev
        names:
          categories:
          - all
          - knative
          - serving
          kind: Configuration
          plural: configurations
          shortNames:
          - config
          - cfg
          singular: configuration
        scope: Namespaced
        subresources:
          status: {}
        versions:
        - name: v1alpha1
          served: true
          storage: true
        - name: v1beta1
          served: true
          storage: false

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          knative.dev/crd-install: "true"
        name: images.caching.internal.knative.dev
      spec:
        group: caching.internal.knative.dev
        names:
          categories:
          - all
          - knative-internal
          - caching
          kind: Image
          plural: images
          shortNames:
          - img
          singular: image
        scope: Namespaced
        subresources:
          status: {}
        version: v1alpha1

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          knative.dev/crd-install: "true"
          serving.knative.dev/release: "v0.7.0"
        name: ingresses.networking.internal.knative.dev
      spec:
        additionalPrinterColumns:
        - JSONPath: .status.conditions[?(@.type=='Ready')].status
          name: Ready
          type: string
        - JSONPath: .status.conditions[?(@.type=='Ready')].reason
          name: Reason
          type: string
        group: networking.internal.knative.dev
        names:
          categories:
          - all
          - knative-internal
          - networking
          kind: Ingress
          plural: ingresses
          shortNames:
          - ing
          singular: ingress
        scope: Namespaced
        subresources:
          status: {}
        version: v1alpha1

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          knative.dev/crd-install: "true"
          serving.knative.dev/release: "v0.7.0"
        name: podautoscalers.autoscaling.internal.knative.dev
      spec:
        additionalPrinterColumns:
        - JSONPath: .status.conditions[?(@.type=='Ready')].status
          name: Ready
          type: string
        - JSONPath: .status.conditions[?(@.type=='Ready')].reason
          name: Reason
          type: string
        group: autoscaling.internal.knative.dev
        names:
          categories:
          - all
          - knative-internal
          - autoscaling
          kind: PodAutoscaler
          plural: podautoscalers
          shortNames:
          - kpa
          singular: podautoscaler
        scope: Namespaced
        subresources:
          status: {}
        version: v1alpha1

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          knative.dev/crd-install: "true"
          serving.knative.dev/release: "v0.7.0"
        name: revisions.serving.knative.dev
      spec:
        additionalPrinterColumns:
        - JSONPath: .status.serviceName
          name: Service Name
          type: string
        - JSONPath: .metadata.labels['serving\.knative\.dev/configurationGeneration']
          name: Generation
          type: string
        - JSONPath: .status.conditions[?(@.type=='Ready')].status
          name: Ready
          type: string
        - JSONPath: .status.conditions[?(@.type=='Ready')].reason
          name: Reason
          type: string
        group: serving.knative.dev
        names:
          categories:
          - all
          - knative
          - serving
          kind: Revision
          plural: revisions
          shortNames:
          - rev
          singular: revision
        scope: Namespaced
        subresources:
          status: {}
        versions:
        - name: v1alpha1
          served: true
          storage: true
        - name: v1beta1
          served: true
          storage: false

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          knative.dev/crd-install: "true"
          serving.knative.dev/release: "v0.7.0"
        name: routes.serving.knative.dev
      spec:
        additionalPrinterColumns:
        - JSONPath: .status.url
          name: URL
          type: string
        - JSONPath: .status.conditions[?(@.type=='Ready')].status
          name: Ready
          type: string
        - JSONPath: .status.conditions[?(@.type=='Ready')].reason
          name: Reason
          type: string
        group: serving.knative.dev
        names:
          categories:
          - all
          - knative
          - serving
          kind: Route
          plural: routes
          shortNames:
          - rt
          singular: route
        scope: Namespaced
        subresources:
          status: {}
        versions:
        - name: v1alpha1
          served: true
          storage: true
        - name: v1beta1
          served: true
          storage: false

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          knative.dev/crd-install: "true"
          serving.knative.dev/release: "v0.7.0"
        name: services.serving.knative.dev
      spec:
        additionalPrinterColumns:
        - JSONPath: .status.url
          name: URL
          type: string
        - JSONPath: .status.latestCreatedRevisionName
          name: LatestCreated
          type: string
        - JSONPath: .status.latestReadyRevisionName
          name: LatestReady
          type: string
        - JSONPath: .status.conditions[?(@.type=='Ready')].status
          name: Ready
          type: string
        - JSONPath: .status.conditions[?(@.type=='Ready')].reason
          name: Reason
          type: string
        group: serving.knative.dev
        names:
          categories:
          - all
          - knative
          - serving
          kind: Service
          plural: services
          shortNames:
          - kservice
          - ksvc
          singular: service
        scope: Namespaced
        subresources:
          status: {}
        versions:
        - name: v1alpha1
          served: true
          storage: true
        - name: v1beta1
          served: true
          storage: false

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          knative.dev/crd-install: "true"
          serving.knative.dev/release: "v0.7.0"
        name: serverlessservices.networking.internal.knative.dev
      spec:
        additionalPrinterColumns:
        - JSONPath: .spec.mode
          name: Mode
          type: string
        - JSONPath: .status.serviceName
          name: ServiceName
          type: string
        - JSONPath: .status.privateServiceName
          name: PrivateServiceName
          type: string
        - JSONPath: .status.conditions[?(@.type=='Ready')].status
          name: Ready
          type: string
        - JSONPath: .status.conditions[?(@.type=='Ready')].reason
          name: Reason
          type: string
        group: networking.internal.knative.dev
        names:
          categories:
          - all
          - knative-internal
          - networking
          kind: ServerlessService
          plural: serverlessservices
          shortNames:
          - sks
          singular: serverlessservice
        scope: Namespaced
        subresources:
          status: {}
        version: v1alpha1

      ---
      apiVersion: v1
      kind: Service
      metadata:
        labels:
          app: activator
          serving.knative.dev/release: "v0.7.0"
        name: activator-service
        namespace: knative-serving
      spec:
        ports:
        - name: http
          port: 80
          protocol: TCP
          targetPort: 8012
        - name: http2
          port: 81
          protocol: TCP
          targetPort: 8013
        - name: metrics
          port: 9090
          protocol: TCP
          targetPort: 9090
        selector:
          app: activator
        type: ClusterIP

      ---
      apiVersion: v1
      kind: Service
      metadata:
        labels:
          app: controller
          serving.knative.dev/release: "v0.7.0"
        name: controller
        namespace: knative-serving
      spec:
        ports:
        - name: metrics
          port: 9090
          protocol: TCP
          targetPort: 9090
        selector:
          app: controller

      ---
      apiVersion: v1
      kind: Service
      metadata:
        labels:
          role: webhook
          serving.knative.dev/release: "v0.7.0"
        name: webhook
        namespace: knative-serving
      spec:
        ports:
        - port: 443
          targetPort: 8443
        selector:
          role: webhook

      ---
      apiVersion: caching.internal.knative.dev/v1alpha1
      kind: Image
      metadata:
        labels:
          serving.knative.dev/release: "v0.7.0"
        name: queue-proxy
        namespace: knative-serving
      spec:
        image: gcr.io/knative-releases/github.com/knative/serving/cmd/queue@sha256:e007c0a78c541600466f88954deee65c517246a23345bfba45a7f212d09b8f3b

      ---
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        labels:
          serving.knative.dev/release: "v0.7.0"
        name: activator
        namespace: knative-serving
      spec:
        selector:
          matchLabels:
            app: activator
            role: activator
        template:
          metadata:
            annotations:
              cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
              sidecar.istio.io/inject: "true"
            labels:
              app: activator
              role: activator
              serving.knative.dev/release: "v0.7.0"
          spec:
            containers:
            - args:
              - -logtostderr=false
              - -stderrthreshold=FATAL
              env:
              - name: POD_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
              - name: SYSTEM_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
              - name: CONFIG_LOGGING_NAME
                value: config-logging
              - name: CONFIG_OBSERVABILITY_NAME
                value: config-observability
              - name: METRICS_DOMAIN
                value: knative.dev/serving
              image: gcr.io/knative-releases/github.com/knative/serving/cmd/activator@sha256:57fe5f1a8b1d12f29fe9e3a904b00c7219e5ce5825d94f33339db929e92257db
              livenessProbe:
                httpGet:
                  httpHeaders:
                  - name: k-kubelet-probe
                    value: activator
                  path: /healthz
                  port: 8012
              name: activator
              ports:
              - containerPort: 8012
                name: http1-port
              - containerPort: 8013
                name: h2c-port
              - containerPort: 9090
                name: metrics-port
              readinessProbe:
                httpGet:
                  httpHeaders:
                  - name: k-kubelet-probe
                    value: activator
                  path: /healthz
                  port: 8012
              resources:
                limits:
                  cpu: 200m
                  memory: 600Mi
                requests:
                  cpu: 20m
                  memory: 60Mi
              securityContext:
                allowPrivilegeEscalation: false
              volumeMounts:
              - mountPath: /etc/config-logging
                name: config-logging
              - mountPath: /etc/config-observability
                name: config-observability
            serviceAccountName: controller
            volumes:
            - configMap:
                name: config-logging
              name: config-logging
            - configMap:
                name: config-observability
              name: config-observability

      ---
      apiVersion: v1
      kind: Service
      metadata:
        labels:
          app: autoscaler
          serving.knative.dev/release: "v0.7.0"
        name: autoscaler
        namespace: knative-serving
      spec:
        ports:
        - name: http
          port: 8080
          protocol: TCP
          targetPort: 8080
        - name: metrics
          port: 9090
          protocol: TCP
          targetPort: 9090
        - name: custom-metrics
          port: 443
          protocol: TCP
          targetPort: 8443
        selector:
          app: autoscaler

      ---
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        labels:
          serving.knative.dev/release: "v0.7.0"
        name: autoscaler
        namespace: knative-serving
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: autoscaler
        template:
          metadata:
            annotations:
              cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
              sidecar.istio.io/inject: "true"
            labels:
              app: autoscaler
              serving.knative.dev/release: "v0.7.0"
          spec:
            containers:
            - args:
              - --secure-port=8443
              - --cert-dir=/tmp
              env:
              - name: SYSTEM_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
              - name: CONFIG_LOGGING_NAME
                value: config-logging
              - name: CONFIG_OBSERVABILITY_NAME
                value: config-observability
              - name: METRICS_DOMAIN
                value: knative.dev/serving
              image: gcr.io/knative-releases/github.com/knative/serving/cmd/autoscaler@sha256:2c2370df2751741348e1cc456f31425cb2455c377ddb45d3f6c17e743fd63d78
              livenessProbe:
                httpGet:
                  httpHeaders:
                  - name: k-kubelet-probe
                    value: autoscaler
                  path: /healthz
                  port: 8080
              name: autoscaler
              ports:
              - containerPort: 8080
                name: websocket
              - containerPort: 9090
                name: metrics
              - containerPort: 8443
                name: custom-metrics
              readinessProbe:
                httpGet:
                  httpHeaders:
                  - name: k-kubelet-probe
                    value: autoscaler
                  path: /healthz
                  port: 8080
              resources:
                limits:
                  cpu: 300m
                  memory: 400Mi
                requests:
                  cpu: 30m
                  memory: 40Mi
              securityContext:
                allowPrivilegeEscalation: false
              volumeMounts:
              - mountPath: /etc/config-autoscaler
                name: config-autoscaler
              - mountPath: /etc/config-logging
                name: config-logging
              - mountPath: /etc/config-observability
                name: config-observability
            serviceAccountName: controller
            volumes:
            - configMap:
                name: config-autoscaler
              name: config-autoscaler
            - configMap:
                name: config-logging
              name: config-logging
            - configMap:
                name: config-observability
              name: config-observability

      ---
      apiVersion: v1
      data:
        _example: |
          ################################
          #                              #
          #    EXAMPLE CONFIGURATION     #
          #                              #
          ################################

          # This block is not actually functional configuration,
          # but serves to illustrate the available configuration
          # options and document them in a way that is accessible
          # to users that `kubectl edit` this config map.
          #
          # These sample configuration options may be copied out of
          # this example block and unindented to be in the data block
          # to actually change the configuration.

          # The Revision ContainerConcurrency field specifies the maximum number
          # of requests the Container can handle at once. Container concurrency
          # target percentage is how much of that maximum to use in a stable
          # state. E.g. if a Revision specifies ContainerConcurrency of 10, then
          # the Autoscaler will try to maintain 7 concurrent connections per pod
          # on average. A value of 70 is chosen because the Autoscaler panics
          # when concurrency exceeds 2x the desired set point. So we will panic
          # before we reach the limit.
          # For legacy and backwards compatibility reasons, this value also accepts
          # fractional values in (0, 1] interval (i.e. 0.7  70%).
          # Thus minimal percentage value must be greater than 1.0, or it will be
          # treated as a fraction.
          # TODO(#2016): Set to 70%.
          container-concurrency-target-percentage: "100"

          # The container concurrency target default is what the Autoscaler will
          # try to maintain when the Revision specifies unlimited concurrency.
          # Even when specifying unlimited concurrency, the autoscaler will
          # horizontally scale the application based on this target concurrency.
          #
          # A value of 100 is chosen because it's enough to allow vertical pod
          # autoscaling to tune resource requests. E.g. maintaining 1 concurrent
          # "hello world" request doesn't consume enough resources to allow VPA
          # to achieve efficient resource usage (VPA CPU minimum is 300m).
          container-concurrency-target-default: "100"

          # When operating in a stable mode, the autoscaler operates on the
          # average concurrency over the stable window.
          stable-window: "60s"

          # When observed average concurrency during the panic window reaches
          # panic-threshold-percentage the target concurrency, the autoscaler
          # enters panic mode. When operating in panic mode, the autoscaler
          # scales on the average concurrency over the panic window which is
          # panic-window-percentage of the stable-window.
          panic-window-percentage: "10.0"

          # Absolute panic window duration.
          # Deprecated in favor of panic-window-percentage.
          # Existing revisions will continue to scale based on panic-window
          # but new revisions will default to panic-window-percentage.
          panic-window: "6s"

          # The percentage of the container concurrency target at which to
          # enter panic mode when reached within the panic window.
          panic-threshold-percentage: "200.0"

          # Max scale up rate limits the rate at which the autoscaler will
          # increase pod count. It is the maximum ratio of desired pods versus
          # observed pods.
          max-scale-up-rate: "10"

          # Scale to zero feature flag
          enable-scale-to-zero: "true"

          # Tick interval is the time between autoscaling calculations.
          tick-interval: "2s"

          # Dynamic parameters (take effect when config map is updated):

          # Scale to zero grace period is the time an inactive revision is left
          # running before it is scaled to zero (min: 30s).
          scale-to-zero-grace-period: "30s"
      kind: ConfigMap
      metadata:
        labels:
          serving.knative.dev/release: "v0.7.0"
        name: config-autoscaler
        namespace: knative-serving

      ---
      apiVersion: v1
      data:
        _example: |
          ################################
          #                              #
          #    EXAMPLE CONFIGURATION     #
          #                              #
          ################################

          # This block is not actually functional configuration,
          # but serves to illustrate the available configuration
          # options and document them in a way that is accessible
          # to users that `kubectl edit` this config map.
          #
          # These sample configuration options may be copied out of
          # this block and unindented to actually change the configuration.

          # IssuerRef is a reference to the issuer for this certificate.
          # IssuerRef should be either `ClusterIssuer` or `Issuer`.
          # Please refer `IssuerRef` in https://github.com/jetstack/cert-manager/blob/master/pkg/apis/certmanager/v1alpha1/types_certificate.go
          # for more details about IssuerRef configuration.
          issuerRef: |
            kind: ClusterIssuer
            name: letsencrypt-issuer

          # solverConfig defines the configuration for the ACME certificate provider.
          # The solverConfig should be either dns01 or http01.
          # Please refer `SolverConfig` in https://github.com/jetstack/cert-manager/blob/master/pkg/apis/certmanager/v1alpha1/types_certificate.go
          # for more details about ACME configuration.
          solverConfig: |
            dns01:
              provider: cloud-dns-provider
      kind: ConfigMap
      metadata:
        labels:
          networking.knative.dev/certificate-provider: cert-manager
          serving.knative.dev/release: "v0.7.0"
        name: config-certmanager
        namespace: knative-serving

      ---
      apiVersion: v1
      data:
        _example: |
          ################################
          #                              #
          #    EXAMPLE CONFIGURATION     #
          #                              #
          ################################

          # This block is not actually functional configuration,
          # but serves to illustrate the available configuration
          # options and document them in a way that is accessible
          # to users that `kubectl edit` this config map.
          #
          # These sample configuration options may be copied out of
          # this example block and unindented to be in the data block
          # to actually change the configuration.

          # revision-timeout-seconds contains the default number of
          # seconds to use for the revision's per-request timeout, if
          # none is specified.
          revision-timeout-seconds: "300"  # 5 minutes

          # max-revision-timeout-seconds contains the maximum number of
          # seconds that can be used for revision-timeout-seconds.
          # This value must be greater than or equal to revision-timeout-seconds.
          # If omitted, the system default is used (600 seconds).
          max-revision-timeout-seconds: "600"  # 10 minutes

          # revision-cpu-request contains the cpu allocation to assign
          # to revisions by default.  If omitted, no value is specified
          # and the system default is used.
          revision-cpu-request: "400m"  # 0.4 of a CPU (aka 400 milli-CPU)

          # revision-memory-request contains the memory allocation to assign
          # to revisions by default.  If omitted, no value is specified
          # and the system default is used.
          revision-memory-request: "100M"  # 100 megabytes of memory

          # revision-cpu-limit contains the cpu allocation to limit
          # revisions to by default.  If omitted, no value is specified
          # and the system default is used.
          revision-cpu-limit: "1000m"  # 1 CPU (aka 1000 milli-CPU)

          # revision-memory-limit contains the memory allocation to limit
          # revisions to by default.  If omitted, no value is specified
          # and the system default is used.
          revision-memory-limit: "200M"  # 200 megabytes of memory

          # container-name-template contains a template for the default
          # container name, if none is specified.  This field supports
          # Go templating and is supplied with the ObjectMeta of the
          # enclosing Service or Configuration, so values such as
          # {{.Name}} are also valid.
          container-name-template: "user-container"
      kind: ConfigMap
      metadata:
        labels:
          serving.knative.dev/release: "v0.7.0"
        name: config-defaults
        namespace: knative-serving

      ---
      apiVersion: v1
      data:
        _example: |
          ################################
          #                              #
          #    EXAMPLE CONFIGURATION     #
          #                              #
          ################################

          # This block is not actually functional configuration,
          # but serves to illustrate the available configuration
          # options and document them in a way that is accessible
          # to users that `kubectl edit` this config map.
          #
          # These sample configuration options may be copied out of
          # this example block and unindented to be in the data block
          # to actually change the configuration.

          # List of repositories for which tag to digest resolving should be skipped
          registriesSkippingTagResolving: "ko.local,dev.local"
        queueSidecarImage: gcr.io/knative-releases/github.com/knative/serving/cmd/queue@sha256:e007c0a78c541600466f88954deee65c517246a23345bfba45a7f212d09b8f3b
      kind: ConfigMap
      metadata:
        labels:
          serving.knative.dev/release: "v0.7.0"
        name: config-deployment
        namespace: knative-serving

      ---
      apiVersion: v1
      data:
        _example: |
          ################################
          #                              #
          #    EXAMPLE CONFIGURATION     #
          #                              #
          ################################

          # This block is not actually functional configuration,
          # but serves to illustrate the available configuration
          # options and document them in a way that is accessible
          # to users that `kubectl edit` this config map.
          #
          # These sample configuration options may be copied out of
          # this example block and unindented to be in the data block
          # to actually change the configuration.

          # Default value for domain.
          # Although it will match all routes, it is the least-specific rule so it
          # will only be used if no other domain matches.
          example.com: |

          # These are example settings of domain.
          # example.org will be used for routes having app=nonprofit.
          example.org: |
            selector:
              app: nonprofit

          # Routes having domain suffix of 'svc.cluster.local' will not be exposed
          # through Ingress. You can define your own label selector to assign that
          # domain suffix to your Route here, or you can set the label
          #    "serving.knative.dev/visibility=cluster-local"
          # to achieve the same effect.  This shows how to make routes having
          # the label app=secret only exposed to the local cluster.
          svc.cluster.local: |
            selector:
              app: secret
      kind: ConfigMap
      metadata:
        labels:
          serving.knative.dev/release: "v0.7.0"
        name: config-domain
        namespace: knative-serving

      ---
      apiVersion: v1
      data:
        _example: |
          ################################
          #                              #
          #    EXAMPLE CONFIGURATION     #
          #                              #
          ################################

          # This block is not actually functional configuration,
          # but serves to illustrate the available configuration
          # options and document them in a way that is accessible
          # to users that `kubectl edit` this config map.
          #
          # These sample configuration options may be copied out of
          # this example block and unindented to be in the data block
          # to actually change the configuration.

          # Delay after revision creation before considering it for GC
          stale-revision-create-delay: "24h"

          # Duration since a route has been pointed at a revision before it should be GC'd
          # This minus lastpinned-debounce be longer than the controller resync period (10 hours)
          stale-revision-timeout: "15h"

          # Minimum number of generations of revisions to keep before considering for GC
          stale-revision-minimum-generations: "1"

          # To avoid constant updates, we allow an existing annotation to be stale by this
          # amount before we update the timestamp
          stale-revision-lastpinned-debounce: "5h"
      kind: ConfigMap
      metadata:
        labels:
          serving.knative.dev/release: "v0.7.0"
        name: config-gc
        namespace: knative-serving

      ---
      apiVersion: v1
      data:
        _example: |
          ################################
          #                              #
          #    EXAMPLE CONFIGURATION     #
          #                              #
          ################################

          # This block is not actually functional configuration,
          # but serves to illustrate the available configuration
          # options and document them in a way that is accessible
          # to users that `kubectl edit` this config map.
          #
          # These sample configuration options may be copied out of
          # this example block and unindented to be in the data block
          # to actually change the configuration.

          # Default Knative Gateway after v0.3. It points to the Istio
          # standard istio-ingressgateway, instead of a custom one that we
          # used pre-0.3.
          gateway.knative-ingress-gateway: "istio-ingressgateway.istio-system.svc.cluster.local"

          # A cluster local gateway to allow pods outside of the mesh to access
          # Services and Routes not exposing through an ingress.  If the users
          # do have a service mesh setup, this isn't required and can be removed.
          #
          # An example use case is when users want to use Istio without any
          # sidecar injection (like Knative's istio-lean.yaml).  Since every pod
          # is outside of the service mesh in that case, a cluster-local  service
          # will need to be exposed to a cluster-local gateway to be accessible.
          local-gateway.cluster-local-gateway: "cluster-local-gateway.istio-system.svc.cluster.local"

          # To use only Istio service mesh and no cluster-local-gateway, replace
          # all local-gateway.* entries the following entry.
          local-gateway.mesh: "mesh"
      kind: ConfigMap
      metadata:
        labels:
          networking.knative.dev/ingress-provider: istio
          serving.knative.dev/release: "v0.7.0"
        name: config-istio
        namespace: knative-serving

      ---
      apiVersion: v1
      data:
        _example: |
          ################################
          #                              #
          #    EXAMPLE CONFIGURATION     #
          #                              #
          ################################

          # This block is not actually functional configuration,
          # but serves to illustrate the available configuration
          # options and document them in a way that is accessible
          # to users that `kubectl edit` this config map.
          #
          # These sample configuration options may be copied out of
          # this example block and unindented to be in the data block
          # to actually change the configuration.

          # Common configuration for all Knative codebase
          zap-logger-config: |
            {
              "level": "info",
              "development": false,
              "outputPaths": ["stdout"],
              "errorOutputPaths": ["stderr"],
              "encoding": "json",
              "encoderConfig": {
                "timeKey": "ts",
                "levelKey": "level",
                "nameKey": "logger",
                "callerKey": "caller",
                "messageKey": "msg",
                "stacktraceKey": "stacktrace",
                "lineEnding": "",
                "levelEncoder": "",
                "timeEncoder": "iso8601",
                "durationEncoder": "",
                "callerEncoder": ""
              }
            }

          # Log level overrides
          # For all components except the autoscaler and queue proxy,
          # changes are be picked up immediately.
          # For autoscaler and queue proxy, changes require recreation of the pods.
          loglevel.controller: "info"
          loglevel.autoscaler: "info"
          loglevel.queueproxy: "info"
          loglevel.webhook: "info"
          loglevel.activator: "info"
      kind: ConfigMap
      metadata:
        labels:
          serving.knative.dev/release: "v0.7.0"
        name: config-logging
        namespace: knative-serving

      ---
      apiVersion: v1
      data:
        _example: |
          ################################
          #                              #
          #    EXAMPLE CONFIGURATION     #
          #                              #
          ################################

          # This block is not actually functional configuration,
          # but serves to illustrate the available configuration
          # options and document them in a way that is accessible
          # to users that `kubectl edit` this config map.
          #
          # These sample configuration options may be copied out of
          # this example block and unindented to be in the data block
          # to actually change the configuration.

          # istio.sidecar.includeOutboundIPRanges specifies the IP ranges that Istio sidecar
          # will intercept.
          #
          # Replace this with the IP ranges of your cluster (see below for some examples).
          # Separate multiple entries with a comma.
          # Example: "10.4.0.0/14,10.7.240.0/20"
          #
          # If set to "*" Istio will intercept all traffic within
          # the cluster as well as traffic that is going outside the cluster.
          # Traffic going outside the cluster will be blocked unless
          # necessary egress rules are created.
          #
          # If omitted or set to "", value of global.proxy.includeIPRanges
          # provided at Istio deployment time is used. In default Knative serving
          # deployment, global.proxy.includeIPRanges value is set to "*".
          #
          # If an invalid value is passed, "" is used instead.
          #
          # If valid set of IP address ranges are put into this value,
          # Istio will no longer intercept traffic going to IP addresses
          # outside the provided ranges and there is no need to specify
          # egress rules.
          #
          # To determine the IP ranges of your cluster:
          #   IBM Cloud Private: cat cluster/config.yaml | grep service_cluster_ip_range
          #   IBM Cloud Kubernetes Service: "172.30.0.0/16,172.20.0.0/16,10.10.10.0/24"
          #   Google Container Engine (GKE): gcloud container clusters describe XXXXXXX --zone=XXXXXX | grep -e clusterIpv4Cidr -e servicesIpv4Cidr
          #   Azure Kubernetes Service (AKS): "10.0.0.0/16"
          #   Azure Container Service (ACS; deprecated): "10.244.0.0/16,10.240.0.0/16"
          #   Azure Container Service Engine (ACS-Engine; OSS): Configurable, but defaults to "10.0.0.0/16"
          #   Minikube: "10.0.0.1/24"
          #
          # For more information, visit
          # https://istio.io/docs/tasks/traffic-management/egress/
          #
          istio.sidecar.includeOutboundIPRanges: "*"

          # clusteringress.class specifies the default cluster ingress class
          # to use when not dictated by Route annotation.
          #
          # If not specified, will use the Istio ingress.
          #
          # Note that changing the ClusterIngress class of an existing Route
          # will result in undefined behavior.  Therefore it is best to only
          # update this value during the setup of Knative, to avoid getting
          # undefined behavior.
          clusteringress.class: "istio.ingress.networking.knative.dev"

          # domainTemplate specifies the golang text template string to use
          # when constructing the Knative service's DNS name. The default
          # value is "{{.Name}}.{{.Namespace}}.{{.Domain}}". And those three
          # values (Name, Namespace, Domain) are the only variables defined.
          #
          # Changing this value might be necessary when the extra levels in
          # the domain name generated is problematic for wildcard certificates
          # that only support a single level of domain name added to the
          # certificate's domain. In those cases you might consider using a value
          # of "{{.Name}}-{{.Namespace}}.{{.Domain}}", or removing the Namespace
          # entirely from the template. When choosing a new value be thoughtful
          # of the potential for conflicts - for example, when users choose to use
          # characters such as `-` in their service, or namespace, names.
          # {{.Annotations}} can be used for any customization in the go template if needed.
          # We strongly recommend keeping namespace part of the template to avoid domain name clashes
          # Example '{{.Name}}-{{.Namespace}}.{{ index .Annotations "sub"}}.{{.Domain}}'
          # and you have an annotation {"sub":"foo"}, then the generated template would be {Name}-{Namespace}.foo.{Domain}
          domainTemplate: "{{.Name}}.{{.Namespace}}.{{.Domain}}"

          # tagTemplate specifies the golang text template string to use
          # when constructing the DNS name for "tags" within the traffic blocks
          # of Routes and Configuration.  This is used in conjunction with the
          # domainTemplate above to determine the full URL for the tag.
          tagTemplate: "{{.Name}}-{{.Tag}}"

          # Controls whether TLS certificates are automatically provisioned and
          # installed in the Knative ingress to terminate external TLS connection.
          # 1. Enabled: enabling auto-TLS feature.
          # 2. Disabled: disabling auto-TLS feature.
          autoTLS: "Disabled"

          # Controls the behavior of the HTTP endpoint for the Knative ingress.
          # It requires autoTLS to be enabled.
          # 1. Enabled: The Knative ingress will be able to serve HTTP connection.
          # 2. Disabled: The Knative ingress ter will reject HTTP traffic.
          # 3. Redirected: The Knative ingress will send a 302 redirect for all
          # http connections, asking the clients to use HTTPS
          httpProtocol: "Enabled"
      kind: ConfigMap
      metadata:
        labels:
          serving.knative.dev/release: "v0.7.0"
        name: config-network
        namespace: knative-serving

      ---
      apiVersion: v1
      data:
        _example: |
          ################################
          #                              #
          #    EXAMPLE CONFIGURATION     #
          #                              #
          ################################

          # This block is not actually functional configuration,
          # but serves to illustrate the available configuration
          # options and document them in a way that is accessible
          # to users that `kubectl edit` this config map.
          #
          # These sample configuration options may be copied out of
          # this example block and unindented to be in the data block
          # to actually change the configuration.

          # logging.enable-var-log-collection defaults to false.
          # The fluentd daemon set will be set up to collect /var/log if
          # this flag is true.
          logging.enable-var-log-collection: false

          # logging.revision-url-template provides a template to use for producing the
          # logging URL that is injected into the status of each Revision.
          # This value is what you might use the the Knative monitoring bundle, and provides
          # access to Kibana after setting up kubectl proxy.
          logging.revision-url-template: |
            http://localhost:8001/api/v1/namespaces/knative-monitoring/services/kibana-logging/proxy/app/kibana#/discover?_a=(query:(match:(kubernetes.labels.knative-dev%2FrevisionUID:(query:'${REVISION_UID}',type:phrase))))

          # If non-empty, this enables queue proxy writing request logs to stdout.
          # The value determines the shape of the request logs and it must be a valid go text/template.
          # It is important to keep this as a single line. Multiple lines are parsed as separate entities
          # by most collection agents and will split the request logs into multiple records.
          #
          # The following fields and functions are available to the template:
          #
          # Request: An http.Request (see https://golang.org/pkg/net/http/#Request)
          # representing an HTTP request received by the server.
          #
          # Response:
          # struct {
          #   Code    int       // HTTP status code (see https://www.iana.org/assignments/http-status-codes/http-status-codes.xhtml)
          #   Size    int       // An int representing the size of the response.
          #   Latency float64   // A float64 representing the latency of the response in seconds.
          # }
          #
          # Revision:
          # struct {
          #   Name          string  // Knative revision name
          #   Namespace     string  // Knative revision namespace
          #   Service       string  // Knative service name
          #   Configuration string  // Knative configuration name
          #   PodName       string  // Name of the pod hosting the revision
          #   PodIP         string  // IP of the pod hosting the revision
          # }
          #
          logging.request-log-template: '{"httpRequest": {"requestMethod": "{{.Request.Method}}", "requestUrl": "{{js .Request.RequestURI}}", "requestSize": "{{.Request.ContentLength}}", "status": {{.Response.Code}}, "responseSize": "{{.Response.Size}}", "userAgent": "{{js .Request.UserAgent}}", "remoteIp": "{{js .Request.RemoteAddr}}", "serverIp": "{{.Revision.PodIP}}", "referer": "{{js .Request.Referer}}", "latency": "{{.Response.Latency}}s", "protocol": "{{.Request.Proto}}"}, "traceId": "{{index .Request.Header "X-B3-Traceid"}}"}'

          # metrics.backend-destination field specifies the system metrics destination.
          # It supports either prometheus (the default) or stackdriver.
          # Note: Using stackdriver will incur additional charges
          metrics.backend-destination: prometheus

          # metrics.request-metrics-backend-destination specifies the request metrics
          # destination. If non-empty, it enables queue proxy to send request metrics.
          # Currently supported values: prometheus, stackdriver.
          metrics.request-metrics-backend-destination: prometheus

          # metrics.stackdriver-project-id field specifies the stackdriver project ID. This
          # field is optional. When running on GCE, application default credentials will be
          # used if this field is not provided.
          metrics.stackdriver-project-id: "<your stackdriver project id>"

          # metrics.allow-stackdriver-custom-metrics indicates whether it is allowed to send metrics to
          # Stackdriver using "global" resource type and custom metric type if the
          # metrics are not supported by "knative_revision" resource type. Setting this
          # flag to "true" could cause extra Stackdriver charge.
          # If metrics.backend-destination is not Stackdriver, this is ignored.
          metrics.allow-stackdriver-custom-metrics: "false"
      kind: ConfigMap
      metadata:
        labels:
          serving.knative.dev/release: "v0.7.0"
        name: config-observability
        namespace: knative-serving

      ---
      apiVersion: v1
      data:
        _example: |
          ################################
          #                              #
          #    EXAMPLE CONFIGURATION     #
          #                              #
          ################################

          # This block is not actually functional configuration,
          # but serves to illustrate the available configuration
          # options and document them in a way that is accessible
          # to users that `kubectl edit` this config map.
          #
          # These sample configuration options may be copied out of
          # this example block and unindented to be in the data block
          # to actually change the configuration.
          #
          # If true we enable adding spans within our applications.
          enable: "false"

          # URL to zipkin collector where traces are sent.
          zipkin-endpoint: "http://zipkin.istio-system.svc.cluster.local:9411/api/v2/spans"

          # Enable zipkin debug mode. This allows all spans to be sent to the server
          # bypassing sampling.
          debug: "false"

          # Percentage (0-1) of requests to trace
          sample-rate: "0.1"
      kind: ConfigMap
      metadata:
        labels:
          serving.knative.dev/release: "v0.7.0"
        name: config-tracing
        namespace: knative-serving

      ---
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        labels:
          serving.knative.dev/release: "v0.7.0"
        name: controller
        namespace: knative-serving
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: controller
        template:
          metadata:
            annotations:
              sidecar.istio.io/inject: "false"
            labels:
              app: controller
              serving.knative.dev/release: "v0.7.0"
          spec:
            containers:
            - env:
              - name: SYSTEM_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
              - name: CONFIG_LOGGING_NAME
                value: config-logging
              - name: CONFIG_OBSERVABILITY_NAME
                value: config-observability
              - name: METRICS_DOMAIN
                value: knative.dev/serving
              image: gcr.io/knative-releases/github.com/knative/serving/cmd/controller@sha256:016c95f2d94be89683d1ddb7ea959667fd2d899087a4145a31d26b5d6f0bb38f
              name: controller
              ports:
              - containerPort: 9090
                name: metrics
              resources:
                limits:
                  cpu: 1000m
                  memory: 1000Mi
                requests:
                  cpu: 100m
                  memory: 100Mi
              securityContext:
                allowPrivilegeEscalation: false
              volumeMounts:
              - mountPath: /etc/config-logging
                name: config-logging
            serviceAccountName: controller
            volumes:
            - configMap:
                name: config-logging
              name: config-logging

      ---
      apiVersion: apiregistration.k8s.io/v1beta1
      kind: APIService
      metadata:
        labels:
          autoscaling.knative.dev/metric-provider: custom-metrics
          serving.knative.dev/release: "v0.7.0"
        name: v1beta1.custom.metrics.k8s.io
      spec:
        group: custom.metrics.k8s.io
        groupPriorityMinimum: 100
        insecureSkipTLSVerify: true
        service:
          name: autoscaler
          namespace: knative-serving
        version: v1beta1
        versionPriority: 100

      ---
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        labels:
          networking.knative.dev/certificate-provider: cert-manager
          serving.knative.dev/release: "v0.7.0"
        name: networking-certmanager
        namespace: knative-serving
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: networking-certmanager
        template:
          metadata:
            annotations:
              sidecar.istio.io/inject: "false"
            labels:
              app: networking-certmanager
          spec:
            containers:
            - env:
              - name: SYSTEM_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
              - name: CONFIG_LOGGING_NAME
                value: config-logging
              - name: CONFIG_OBSERVABILITY_NAME
                value: config-observability
              - name: METRICS_DOMAIN
                value: knative.dev/serving
              image: gcr.io/knative-releases/github.com/knative/serving/cmd/networking/certmanager@sha256:c757629165393f778d5c0e8b611c9c4857b24f0c748d985d3a080d0161a85248
              name: networking-certmanager
              ports:
              - containerPort: 9090
                name: metrics
              resources:
                limits:
                  cpu: 1000m
                  memory: 1000Mi
                requests:
                  cpu: 100m
                  memory: 100Mi
              securityContext:
                allowPrivilegeEscalation: false
              volumeMounts:
              - mountPath: /etc/config-logging
                name: config-logging
            serviceAccountName: controller
            volumes:
            - configMap:
                name: config-logging
              name: config-logging

      ---
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        labels:
          networking.knative.dev/ingress-provider: istio
          serving.knative.dev/release: "v0.7.0"
        name: networking-istio
        namespace: knative-serving
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: networking-istio
        template:
          metadata:
            annotations:
              sidecar.istio.io/inject: "false"
            labels:
              app: networking-istio
          spec:
            containers:
            - env:
              - name: SYSTEM_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
              - name: CONFIG_LOGGING_NAME
                value: config-logging
              - name: CONFIG_OBSERVABILITY_NAME
                value: config-observability
              - name: METRICS_DOMAIN
                value: knative.dev/serving
              image: gcr.io/knative-releases/github.com/knative/serving/cmd/networking/istio@sha256:bb4407e4714511cd9429e86536c283265629a2c11c80633d91c0f798c494a16f
              name: networking-istio
              ports:
              - containerPort: 9090
                name: metrics
              resources:
                limits:
                  cpu: 1000m
                  memory: 1000Mi
                requests:
                  cpu: 100m
                  memory: 100Mi
              securityContext:
                allowPrivilegeEscalation: false
              volumeMounts:
              - mountPath: /etc/config-logging
                name: config-logging
            serviceAccountName: controller
            volumes:
            - configMap:
                name: config-logging
              name: config-logging

      ---
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        labels:
          serving.knative.dev/release: "v0.7.0"
        name: webhook
        namespace: knative-serving
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: webhook
            role: webhook
        template:
          metadata:
            annotations:
              cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
              sidecar.istio.io/inject: "false"
            labels:
              app: webhook
              role: webhook
              serving.knative.dev/release: "v0.7.0"
          spec:
            containers:
            - env:
              - name: SYSTEM_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
              - name: CONFIG_LOGGING_NAME
                value: config-logging
              image: gcr.io/knative-releases/github.com/knative/serving/cmd/webhook@sha256:d9918d40492e0b20b48576ff6182e2ab896e50dfd2313cb471419be98f821b9c
              name: webhook
              resources:
                limits:
                  cpu: 200m
                  memory: 200Mi
                requests:
                  cpu: 20m
                  memory: 20Mi
              securityContext:
                allowPrivilegeEscalation: false
              volumeMounts:
              - mountPath: /etc/config-logging
                name: config-logging
            serviceAccountName: controller
            volumes:
            - configMap:
                name: config-logging
              name: config-logging

      ---
    name: knative-serving
    path: https://storage.googleapis.com/knative-releases/serving/previous/v0.7.0/serving.yaml
  - content: |
      apiVersion: v1
      kind: Namespace
      metadata:
        labels:
          istio-injection: enabled
          projectriff.io/release: "0.1.0-snapshot"
        name: riff-system

      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        labels:
          projectriff.io/release: "0.1.0-snapshot"
          rbac.authorization.k8s.io/aggregate-to-admin: "true"
          rbac.authorization.k8s.io/aggregate-to-edit: "true"
        name: projectriff
      rules:
      - apiGroups:
        - build.projectriff.io
        resources:
        - '*'
        verbs:
        - '*'
      - apiGroups:
        - request.projectriff.io
        resources:
        - '*'
        verbs:
        - '*'
      - apiGroups:
        - stream.projectriff.io
        resources:
        - '*'
        verbs:
        - '*'
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        labels:
          projectriff.io/release: "0.1.0-snapshot"
          rbac.authorization.k8s.io/aggregate-to-view: "true"
        name: projectriff-readonly
      rules:
      - apiGroups:
        - build.projectriff.io
        resources:
        - '*'
        verbs:
        - get
        - list
        - watch
      - apiGroups:
        - request.projectriff.io
        resources:
        - '*'
        verbs:
        - get
        - list
        - watch
      - apiGroups:
        - stream.projectriff.io
        resources:
        - '*'
        verbs:
        - get
        - list
        - watch
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        labels:
          projectriff.io/release: "0.1.0-snapshot"
        name: projectriff-admin
      rules:
      - apiGroups:
        - build.projectriff.io
        resources:
        - '*'
        verbs:
        - '*'
      - apiGroups:
        - request.projectriff.io
        resources:
        - '*'
        verbs:
        - '*'
      - apiGroups:
        - stream.projectriff.io
        resources:
        - '*'
        verbs:
        - '*'
      - apiGroups:
        - ""
        resources:
        - secrets
        - configmaps
        - serviceaccounts
        - persistentvolumeclaims
        verbs:
        - '*'
      - apiGroups:
        - apps
        resources:
        - deployments
        - deployments/finalizers
        verbs:
        - '*'
      - apiGroups:
        - serving.knative.dev
        resources:
        - '*'
        verbs:
        - '*'
      - apiGroups:
        - build.knative.dev
        resources:
        - '*'
        verbs:
        - '*'
      - apiGroups:
        - admissionregistration.k8s.io
        resources:
        - mutatingwebhookconfigurations
        verbs:
        - '*'

      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        labels:
          projectriff.io/release: "0.1.0-snapshot"
        name: controller
        namespace: riff-system

      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        labels:
          projectriff.io/release: "0.1.0-snapshot"
        name: riff-system-controller-admin
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: projectriff-admin
      subjects:
      - kind: ServiceAccount
        name: controller
        namespace: riff-system

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          projectriff.io/release: "0.1.0-snapshot"
        name: applications.build.projectriff.io
      spec:
        additionalPrinterColumns:
        - JSONPath: .status.conditions[?(@.type=="Ready")].status
          name: Ready
          type: string
        - JSONPath: .status.conditions[?(@.type=="Ready")].reason
          name: Reason
          type: string
        group: build.projectriff.io
        names:
          categories:
          - riff
          kind: Application
          plural: applications
          singular: application
        scope: Namespaced
        subresources:
          status: {}
        version: v1alpha1

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          projectriff.io/release: "0.1.0-snapshot"
        name: functions.build.projectriff.io
      spec:
        additionalPrinterColumns:
        - JSONPath: .status.conditions[?(@.type=="Ready")].status
          name: Ready
          type: string
        - JSONPath: .status.conditions[?(@.type=="Ready")].reason
          name: Reason
          type: string
        group: build.projectriff.io
        names:
          categories:
          - riff
          kind: Function
          plural: functions
          singular: function
        scope: Namespaced
        subresources:
          status: {}
        version: v1alpha1

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          projectriff.io/release: "0.1.0-snapshot"
        name: handlers.request.projectriff.io
      spec:
        additionalPrinterColumns:
        - JSONPath: .status.url
          name: URL
          type: string
        - JSONPath: .status.conditions[?(@.type=="Ready")].status
          name: Ready
          type: string
        - JSONPath: .status.conditions[?(@.type=="Ready")].reason
          name: Reason
          type: string
        group: request.projectriff.io
        names:
          categories:
          - riff
          kind: Handler
          plural: handlers
          singular: handler
        scope: Namespaced
        subresources:
          status: {}
        version: v1alpha1

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          projectriff.io/release: "0.1.0-snapshot"
        name: processors.stream.projectriff.io
      spec:
        additionalPrinterColumns:
        - JSONPath: .status.conditions[?(@.type=="Ready")].status
          name: Ready
          type: string
        - JSONPath: .status.conditions[?(@.type=="Ready")].reason
          name: Reason
          type: string
        group: stream.projectriff.io
        names:
          categories:
          - riff
          kind: Processor
          plural: processors
          singular: processor
        scope: Namespaced
        subresources:
          status: {}
        version: v1alpha1

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        labels:
          projectriff.io/release: "0.1.0-snapshot"
        name: streams.stream.projectriff.io
      spec:
        additionalPrinterColumns:
        - JSONPath: .status.conditions[?(@.type=="Ready")].status
          name: Ready
          type: string
        - JSONPath: .status.conditions[?(@.type=="Ready")].reason
          name: Reason
          type: string
        group: stream.projectriff.io
        names:
          categories:
          - riff
          kind: Stream
          plural: streams
          singular: stream
        scope: Namespaced
        subresources:
          status: {}
        version: v1alpha1

      ---
      apiVersion: v1
      kind: Service
      metadata:
        labels:
          app: controller
          projectriff.io/release: "0.1.0-snapshot"
        name: controller
        namespace: riff-system
      spec:
        ports:
        - name: metrics
          port: 9090
          protocol: TCP
          targetPort: 9090
        selector:
          app: controller

      ---
      apiVersion: v1
      kind: Service
      metadata:
        labels:
          projectriff.io/release: "0.1.0-snapshot"
          role: webhook
        name: webhook
        namespace: riff-system
      spec:
        ports:
        - port: 443
          targetPort: 443
        selector:
          role: webhook

      ---
      apiVersion: v1
      data:
        _example: |
          ################################
          #                              #
          #    EXAMPLE CONFIGURATION     #
          #                              #
          ################################

          # This block is not actually functional configuration,
          # but serves to illustrate the available configuration
          # options and document them in a way that is accessible
          # to users that `kubectl edit` this config map.
          #
          # These sample configuration options may be copied out of
          # this block and unindented to actually change the configuration.
      kind: ConfigMap
      metadata:
        labels:
          projectriff.io/release: "0.1.0-snapshot"
        name: config-controller
        namespace: riff-system

      ---
      apiVersion: v1
      data:
        _example: |
          ################################
          #                              #
          #    EXAMPLE CONFIGURATION     #
          #                              #
          ################################

          # This block is not actually functional configuration,
          # but serves to illustrate the available configuration
          # options and document them in a way that is accessible
          # to users that `kubectl edit` this config map.
          #
          # These sample configuration options may be copied out of
          # this block and unindented to actually change the configuration.

          # Common configuration for all Knative codebase
          zap-logger-config: |
            {
              "level": "info",
              "development": false,
              "outputPaths": ["stdout"],
              "errorOutputPaths": ["stderr"],
              "encoding": "json",
              "encoderConfig": {
                "timeKey": "ts",
                "levelKey": "level",
                "nameKey": "logger",
                "callerKey": "caller",
                "messageKey": "msg",
                "stacktraceKey": "stacktrace",
                "lineEnding": "",
                "levelEncoder": "",
                "timeEncoder": "iso8601",
                "durationEncoder": "",
                "callerEncoder": ""
              }
            }

          # Log level overrides
          # For all components except the autoscaler and queue proxy,
          # changes are be picked up immediately.
          # For autoscaler and queue proxy, changes require recreation of the pods.
          loglevel.controller: "info"
          loglevel.autoscaler: "info"
          loglevel.queueproxy: "info"
          loglevel.webhook: "info"
          loglevel.activator: "info"
      kind: ConfigMap
      metadata:
        labels:
          projectriff.io/release: "0.1.0-snapshot"
        name: config-logging
        namespace: riff-system

      ---
      apiVersion: v1
      data:
        _example: |
          ################################
          #                              #
          #    EXAMPLE CONFIGURATION     #
          #                              #
          ################################

          # This block is not actually functional configuration,
          # but serves to illustrate the available configuration
          # options and document them in a way that is accessible
          # to users that `kubectl edit` this config map.
          #
          # These sample configuration options may be copied out of
          # this block and unindented to actually change the configuration.

          # logging.enable-var-log-collection defaults to false.
          # A fluentd sidecar will be set up to collect var log if
          # this flag is true.
          logging.enable-var-log-collection: false

          # logging.fluentd-sidecar-image provides the fluentd sidecar image
          # to inject as a sidecar to collect logs from /var/log.
          # Must be presented if logging.enable-var-log-collection is true.
          logging.fluentd-sidecar-image: k8s.gcr.io/fluentd-elasticsearch:v2.0.4

          # logging.fluentd-sidecar-output-config provides the configuration
          # for the fluentd sidecar, which will be placed into a configmap and
          # mounted into the fluentd sidecar image.
          logging.fluentd-sidecar-output-config: |
            # Parse json log before sending to Elastic Search
            <filter **>
              @type parser
              key_name log
              <parse>
                @type multi_format
                <pattern>
                  format json
                  time_key fluentd-time # fluentd-time is reserved for structured logs
                  time_format %Y-%m-%dT%H:%M:%S.%NZ
                </pattern>
                <pattern>
                  format none
                  message_key log
                </pattern>
              </parse>
            </filter>
            # Send to Elastic Search
            <match **>
              @id elasticsearch
              @type elasticsearch
              @log_level info
              include_tag_key true
              # Elasticsearch service is in monitoring namespace.
              host elasticsearch-logging.knative-monitoring
              port 9200
              logstash_format true
              <buffer>
                @type file
                path /var/log/fluentd-buffers/kubernetes.system.buffer
                flush_mode interval
                retry_type exponential_backoff
                flush_thread_count 2
                flush_interval 5s
                retry_forever
                retry_max_interval 30
                chunk_limit_size 2M
                queue_limit_length 8
                overflow_action block
              </buffer>
            </match>

          # logging.revision-url-template provides a template to use for producing the
          # logging URL that is injected into the status of each Revision.
          # This value is what you might use the the Knative monitoring bundle, and provides
          # access to Kibana after setting up kubectl proxy.
          logging.revision-url-template: |
            http://localhost:8001/api/v1/namespaces/knative-monitoring/services/kibana-logging/proxy/app/kibana#/discover?_a=(query:(match:(kubernetes.labels.knative-dev%2FrevisionUID:(query:'${REVISION_UID}',type:phrase))))

          # metrics.backend-destination field specifies the system metrics destination.
          # It supports either prometheus (the default) or stackdriver.
          # Note: Using stackdriver will incur additional charges
          metrics.backend-destination: prometheus

          # metrics.stackdriver-project-id field specifies the stackdriver project ID. This
          # field is optional. When running on GCE, application default credentials will be
          # used if this field is not provided.
          metrics.stackdriver-project-id: "<your stackdriver project id>"
      kind: ConfigMap
      metadata:
        labels:
          projectriff.io/release: "0.1.0-snapshot"
        name: config-observability
        namespace: riff-system

      ---
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        labels:
          projectriff.io/release: "0.1.0-snapshot"
        name: controller
        namespace: riff-system
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: controller
        template:
          metadata:
            annotations:
              sidecar.istio.io/inject: "false"
            labels:
              app: controller
          spec:
            containers:
            - env:
              - name: METRICS_DOMAIN
                value: projectriff.io/system
              - name: SYSTEM_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
              image: gcr.io/projectriff/system/github.com/projectriff/system/cmd/controller@sha256:c4d546d44504fc33d0d6acb8a1e111475f21f43eaad5871eca9d1e1a9a283fd0
              name: controller
              ports:
              - containerPort: 9090
                name: metrics
              resources:
                limits:
                  cpu: 1000m
                  memory: 1000Mi
                requests:
                  cpu: 100m
                  memory: 100Mi
              volumeMounts:
              - mountPath: /etc/config-logging
                name: config-logging
            serviceAccountName: controller
            volumes:
            - configMap:
                name: config-logging
              name: config-logging

      ---
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        labels:
          projectriff.io/release: "0.1.0-snapshot"
        name: webhook
        namespace: riff-system
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: webhook
            role: webhook
        template:
          metadata:
            annotations:
              sidecar.istio.io/inject: "false"
            labels:
              app: webhook
              role: webhook
          spec:
            containers:
            - env:
              - name: SYSTEM_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
              image: gcr.io/projectriff/system/github.com/projectriff/system/cmd/webhook@sha256:f395749cf7e47ebc304c7cd03b8b597da5318a3c66e1c6b6e5da2ab71f487db3
              name: webhook
              resources:
                limits:
                  cpu: 200m
                  memory: 200Mi
                requests:
                  cpu: 20m
                  memory: 20Mi
              volumeMounts:
              - mountPath: /etc/config-logging
                name: config-logging
            serviceAccountName: controller
            volumes:
            - configMap:
                name: config-logging
              name: config-logging

      ---
    name: riff-system
    path: https://storage.googleapis.com/projectriff/riff-system/riff-system-0.1.0-snapshot.yaml
  - content: |
      apiVersion: build.knative.dev/v1alpha1
      kind: ClusterBuildTemplate
      metadata:
        name: riff-application
      spec:
        parameters:
        - name: IMAGE
          description: The image you wish to create. For example, "repo/example", or "example.com/repo/image".
        - name: BUILDER_IMAGE
          description: The builder image (must include v3 lifecycle and compatible buildpacks).
          default: docker.io/cloudfoundry/cnb:0.0.5-bionic@sha256:93e2707b756ed9544ecccab6884424e4990e2b3ec08f0435b8249499b51b2354
        - name: USE_CRED_HELPERS
          description: Use Docker credential helpers for Google's GCR, Amazon's ECR, or Microsoft's ACR.
          default: 'false'
        - name: CACHE
          description: The name of the persistent app cache volume
          default: empty-dir
        - name: USER_ID
          description: The user ID of the builder image user
          default: "1000"
        - name: GROUP_ID
          description: The group ID of the builder image user
          default: "1000"

        steps:
        - name: prepare
          image: docker.io/library/ubuntu:18.04@sha256:eb70667a801686f914408558660da753cde27192cd036148e58258819b927395
          command: ["/bin/sh"]
          args:
          - "-c"
          - >
            chown -R "${USER_ID}:${GROUP_ID}" "/builder/home" &&
            chown -R "${USER_ID}:${GROUP_ID}" /layers &&
            chown -R "${USER_ID}:${GROUP_ID}" /cache &&
            chown -R "${USER_ID}:${GROUP_ID}" /workspace
          imagePullPolicy: Always
          volumeMounts:
          - name: "layers-dir"
            mountPath: /layers
          - name: "${CACHE}"
            mountPath: /cache

        - name: detect
          image: ${BUILDER_IMAGE}
          command: ["/lifecycle/detector"]
          args:
          - "-app=/workspace"
          - "-group=/layers/group.toml"
          - "-plan=/layers/plan.toml"
          imagePullPolicy: Always
          volumeMounts:
          - name: "layers-dir"
            mountPath: /layers

        - name: restore
          image: ${BUILDER_IMAGE}
          command: ["/lifecycle/restorer"]
          args:
          - "-group=/layers/group.toml"
          - "-layers=/layers"
          - "-path=/cache"
          imagePullPolicy: Always
          volumeMounts:
          - name: "${CACHE}"
            mountPath: /cache
          - name: "layers-dir"
            mountPath: /layers

        - name: analyze
          image: ${BUILDER_IMAGE}
          command: ["/lifecycle/analyzer"]
          args:
          - "-layers=/layers"
          - "-analyzed=/layers/analyzed.toml"
          - "-helpers=${USE_CRED_HELPERS}"
          - "-group=/layers/group.toml"
          - "${IMAGE}"
          imagePullPolicy: Always
          volumeMounts:
          - name: "layers-dir"
            mountPath: /layers

        - name: build
          image: ${BUILDER_IMAGE}
          command: ["/lifecycle/builder"]
          args:
          - "-layers=/layers"
          - "-app=/workspace"
          - "-group=/layers/group.toml"
          - "-plan=/layers/plan.toml"
          imagePullPolicy: Always
          volumeMounts:
          - name: "layers-dir"
            mountPath: /layers

        - name: export
          image: ${BUILDER_IMAGE}
          command: ["/lifecycle/exporter"]
          args:
          - "-layers=/layers"
          - "-analyzed=/layers/analyzed.toml"
          - "-helpers=${USE_CRED_HELPERS}"
          - "-app=/workspace"
          - "-group=/layers/group.toml"
          - "${IMAGE}"
          imagePullPolicy: Always
          volumeMounts:
          - name: "layers-dir"
            mountPath: /layers

        - name: cache
          image: ${BUILDER_IMAGE}
          command: ["/lifecycle/cacher"]
          args:
          - "-group=/layers/group.toml"
          - "-layers=/layers"
          - "-path=/cache"
          imagePullPolicy: Always
          volumeMounts:
          - name: "${CACHE}"
            mountPath: /cache
          - name: "layers-dir"
            mountPath: /layers

        volumes:
        - name: empty-dir
          emptyDir: {}
        - name: layers-dir
          emptyDir: {}
    name: riff-application-build-template
    path: https://storage.googleapis.com/projectriff/riff-buildtemplate/riff-application-clusterbuildtemplate-0.3.0-snapshot.yaml
  - content: |
      apiVersion: build.knative.dev/v1alpha1
      kind: ClusterBuildTemplate
      metadata:
        name: riff-function
      spec:
        parameters:
        - name: IMAGE
          description: The image you wish to create. For example, "repo/example", or "example.com/repo/image".
        - name: FUNCTION_ARTIFACT
          default: ""
          description: Path to the function artifact, source code or jar file (attempts
            detection if not specified)
        - name: FUNCTION_HANDLER
          default: ""
          description: Name of method or class to invoke (see specific invoker for detail)
        - name: FUNCTION_LANGUAGE
          default: ""
          description: Explicit language to use for the function, will skip detection.
        - name: BUILDER_IMAGE
          description: The builder image (must include v3 lifecycle and compatible buildpacks).
          default: docker.io/projectriff/builder:0.3.0-snapshot@sha256:e3fcbcd4329d4599dffb805580672b4da124f1116d081af97a4b0d791a51efc2
        - name: USE_CRED_HELPERS
          description: Use Docker credential helpers for Google's GCR, Amazon's ECR, or Microsoft's ACR.
          default: 'false'
        - name: CACHE
          description: The name of the persistent app cache volume
          default: empty-dir
        - name: USER_ID
          description: The user ID of the builder image user
          default: "1000"
        - name: GROUP_ID
          description: The group ID of the builder image user
          default: "1000"

        steps:
        - name: prepare
          image: docker.io/library/ubuntu:18.04@sha256:eb70667a801686f914408558660da753cde27192cd036148e58258819b927395
          command: ["/bin/sh"]
          args:
          - "-c"
          - >
            chown -R "${USER_ID}:${GROUP_ID}" "/builder/home" &&
            chown -R "${USER_ID}:${GROUP_ID}" /layers &&
            chown -R "${USER_ID}:${GROUP_ID}" /cache &&
            chown -R "${USER_ID}:${GROUP_ID}" /workspace
          imagePullPolicy: Always
          volumeMounts:
          - name: "layers-dir"
            mountPath: /layers
          - name: "${CACHE}"
            mountPath: /cache

        - name: detect
          image: ${BUILDER_IMAGE}
          command: ["/lifecycle/detector"]
          args:
          - "-app=/workspace"
          - "-group=/layers/group.toml"
          - "-plan=/layers/plan.toml"
          env:
          - name: RIFF
            value: "true"
          - name: RIFF_ARTIFACT
            value: ${FUNCTION_ARTIFACT}
          - name: RIFF_HANDLER
            value: ${FUNCTION_HANDLER}
          - name: RIFF_OVERRIDE
            value: ${FUNCTION_LANGUAGE}
          imagePullPolicy: Always
          volumeMounts:
          - name: "layers-dir"
            mountPath: /layers

        - name: restore
          image: ${BUILDER_IMAGE}
          command: ["/lifecycle/restorer"]
          args:
          - "-group=/layers/group.toml"
          - "-layers=/layers"
          - "-path=/cache"
          imagePullPolicy: Always
          volumeMounts:
          - name: "${CACHE}"
            mountPath: /cache
          - name: "layers-dir"
            mountPath: /layers

        - name: analyze
          image: ${BUILDER_IMAGE}
          command: ["/lifecycle/analyzer"]
          args:
          - "-layers=/layers"
          - "-analyzed=/layers/analyzed.toml"
          - "-helpers=${USE_CRED_HELPERS}"
          - "-group=/layers/group.toml"
          - "${IMAGE}"
          imagePullPolicy: Always
          volumeMounts:
          - name: "layers-dir"
            mountPath: /layers

        - name: build
          image: ${BUILDER_IMAGE}
          command: ["/lifecycle/builder"]
          args:
          - "-layers=/layers"
          - "-app=/workspace"
          - "-group=/layers/group.toml"
          - "-plan=/layers/plan.toml"
          env:
          - name: RIFF
            value: "true"
          - name: RIFF_ARTIFACT
            value: ${FUNCTION_ARTIFACT}
          - name: RIFF_HANDLER
            value: ${FUNCTION_HANDLER}
          - name: RIFF_OVERRIDE
            value: ${FUNCTION_LANGUAGE}
          imagePullPolicy: Always
          volumeMounts:
          - name: "layers-dir"
            mountPath: /layers

        - name: export
          image: ${BUILDER_IMAGE}
          command: ["/lifecycle/exporter"]
          args:
          - "-layers=/layers"
          - "-analyzed=/layers/analyzed.toml"
          - "-helpers=${USE_CRED_HELPERS}"
          - "-app=/workspace"
          - "-group=/layers/group.toml"
          - "${IMAGE}"
          imagePullPolicy: Always
          volumeMounts:
          - name: "layers-dir"
            mountPath: /layers

        - name: cache
          image: ${BUILDER_IMAGE}
          command: ["/lifecycle/cacher"]
          args:
          - "-group=/layers/group.toml"
          - "-layers=/layers"
          - "-path=/cache"
          imagePullPolicy: Always
          volumeMounts:
          - name: "${CACHE}"
            mountPath: /cache
          - name: "layers-dir"
            mountPath: /layers

        volumes:
        - name: empty-dir
          emptyDir: {}
        - name: layers-dir
          emptyDir: {}
    name: riff-function-build-template
    path: https://storage.googleapis.com/projectriff/riff-buildtemplate/riff-function-clusterbuildtemplate-0.3.0-snapshot.yaml
status: {}
